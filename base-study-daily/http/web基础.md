##### 输入url后发生了什么？

>首先从DNS（Domain Name Server，域名服务器）解析域名获取ip地址，根据ip地址找到服务器，服务器根据地址请求，返回相关的数据，浏览器获取到数据进行页面加载渲染页面
>
>- DNS域名解析： 在TCP/IP协议的第四层应用层，基于UDP
>
>  浏览器查找域名的 IP 地址 这一步包括 DNS 具体的查找过程：浏览器缓存->系统缓存->hosts->DNS。
>
>  - 浏览器会检查本地缓存中有没有这个域名：使用 `chrome://net-internals/#dns` 来进行查看
>  - 搜索操作系统自身的DNS缓存，如果找到并且没有过期就停止搜索
>  - 尝试读取hosts文件，看这里面有没有该域名对应的IP地址
>  - 浏览器发起一个DNS系统调用，就会向本地配置的首选DNS服务器发起域名解析请求，运营商的DNS服务器首先查找自身的缓存，找到对应的条目，且没有过期，则解析成功
>
>- TCP三次握手：建立TCP/IP连接
>
>  拿到域名对应的IP地址之后，浏览器会以一个随机端口（1024 < 端口 < 65535）向服务器的Web server 80端口发起TCP的连接请求。（常见的web server产品有 apache、nginx、IIS、Lighttpd 等）
>
>  这个连接请求到达服务器端后，进入到网卡，然后是进入到内核的TCP/IP协议栈，还有可能要经过防火墙的过滤，最终到达WEB程序，最终建立了TCP/IP的连接
>
>  三次握手：
>
>  - 客户端–发送带有SYN标志的数据包–一次握手–服务端     （“喂，你听得到吗？”）
>  - 服务端–发送带有SYN/ACK标志的数据包–二次握手–客户端  （“我听得到呀，你听得到我吗？”）
>  - 客户端–发送带有带有ACK标志的数据包–三次握手–服务端  （“我能听到你，今天 balabala……” ）
>
>- 建立TCP连接后，浏览器发起http请求
>
>- 服务端响应http请求，浏览器得到html代码
>
>  浏览器通常使用 MIME type(Content-Type) 替代文件扩展名来确定文档类型，因此服务器在响应头中设置正确的 MIME type 是非常重要的
>
>- 浏览器渲染
>
>  - 通过后台返回的HTML字符串被浏览器解析，html页面经过加载、解析、渲染。
>  - HTML解析 -------->DOM树
>  - CSS 解析  -------->样式树
>  - 两者结合   -------->渲染树
>  - 布局
>  - 像素绘制页面
>
>- 延伸问题：为什么需要DNS解析域名为IP地址？
>
>  网络通讯大部分是基于TCP/IP的，而TCP/IP是基于IP地址的，所以计算机在网络上进行通讯时只能识别如“202.96.134.133”之类的IP地址，而不能认识域名。我们无法记住10个以上IP地址的网站，所以我们访问网站时，更多的是在浏览器地址栏中输入域名，就能看到所需要的页面，这是因为有一个叫“DNS服务器”的计算机自动把我们的域名“翻译”成了相应的IP地址，然后调出IP地址所对应的网页
>
>- 延伸问题：MIMIE Type（Content-Type）？
>
>  `MIME`是多用途 Internet 邮件扩展（`Multi-purpose Internet Mail Extensions`）的首字母缩写。 使用标准化的方式来表示网络之间传输的文档类型及格式
>
>  - MIME type 由两部分组成：斜杠（/）分隔的类型和子类型，中间无空格。例如：Microsoft Word 文件的 MIME type 是 `application/msword`，即类型是 application，子类型是 msword。
>  - 浏览器通常使用 MIME type 替代文件扩展名来确定文档类型，因此服务器在响应头中设置正确的 MIME type 是非常重要的。
>  - MIME type 对大小写不敏感，但是一般都使用小写。
>  - 对于 `text`类型若没有指定其子类型就使用 `text/plain`；对于二进制文件没有指定其子类型就使用 `application/octet-stream`。
>  - 所有的 `text/*script*`类型已被废弃。
>  - 当 MIME type 缺失或错误时，浏览器可能会查看资源以确定文件类型。我们可以通过设置 `X-Content-Type-Options`为 `nosniff`来阻止浏览器对 MIME type 的嗅探。
>  - 不同类型的文件可以通过查看二进制来判断其类型，但并非所有文件都如此。如：PNG 文件头标识 (8 bytes)   89 50 4E 47 0D 0A 1A 0A；GIF 文件头标识 (6 bytes)   47 49 46 38 39(37) 61
>
>- 延伸问题：gzip 是做什么的？
>
>  - 在 `http`传输时，浏览器用以解压文件，但浏览器怎么分辨文件是什么格式，应该用什么格式去解压呢？
>
>  - `http／1.0`协议中关于服务端发送的数据可以配置一个 `Content-Encoding`字段，这个字段用于说明数据的压缩方法：`Content-Encoding: gzip,Content-Encoding: compress,Content-Encoding: deflate`
>
>  - 客户端在接受到返回的数据后去检查对应字段的信息，然后根据对应的格式去做相应的解码。客户端在请求时，可以用 `Accept-Encoding`字段说明自己接受哪些压缩方法:`Accept-Encoding: gzip, deflate`
>
>- 延伸问题：HTTP 协议中的 Transfer-Encoding
>
>  Transfer-Encoding，是一个 HTTP 头部字段，字面意思是「传输编码」。实际上，HTTP 协议中还有另外一个头部与编码有关：Content-Encoding（内容编码）。Content-Encoding 通常用于对实体内容进行压缩编码，目的是优化传输，例如用 gzip 压缩文本文件，能大幅减小体积。内容编码通常是选择性的，例如 jpg / png 这类文件一般不开启，因为图片格式已经是高度压缩过的，再压一遍没什么效果不说还浪费 CPU。
>
>  而 Transfer-Encoding 则是用来改变报文格式，它不但不会减少实体内容传输大小，甚至还会使传输变大，那它的作用是什么呢？本文接下来主要就是讲这个。我们先记住一点，Content-Encoding 和 Transfer-Encoding 二者是相辅相成的，对于一个 HTTP 报文，很可能同时进行了内容编码和传输编码。
>
>  分块编码，头部加入 `Transfer-Encoding: chunked`。报文中的实体需要改为用一系列分块来传输

##### 七层OSI模型和TCP五层模型

>- OSI七层模型从上到下依次是：
>
>  应用层：为应用程序提供网络服务；文件传输；                  TFTP、HTTP、SNMP、FTP、SMTP、DNS
>
>  表示层：数据格式转换、数据压缩和数据加密；                  没有协议
>
>  会话层：建立、断开、维护通信链接；                               没有协议
>
>  ----------------应用层
>
>  传输层： 为上层协议提供端到端的可靠传输；                    UDP、TCP
>
>  网络层： 寻址和路由；                                                   IP、ARP、ICMP、RTP、OSPF、BGP
>
>  数据链路层：定义通过通信媒介互连的设备之间传输规范；   SLIP、CSLIP、PPP、ARP
>
>  物理层： 利用物理传输介质为数据链路层提供物理链接         ISO2110、IEEE802、 IEEE802.2
>
>- TCP五层模型相比OSI七层模型，将OSI的应用层、表示层和会话层合为一层：应用层，其他不变
>
>- IP协议（网络层）的作用
>
>  - 把各种数据包传送给对方。而要保证确实传送到对方那里，则需要满足各类条件。其中两个重要的条件是 IP 地址和 MAC地址（Media Access Control Address）
>
>  - IP 地址指明了节点被分配到的地址，MAC 地址是指网卡所属的固定地址。IP 地址可以和 MAC 地址进行配对。IP 地址可变换，但 MAC地址基本上不会更改
>
>  使用 ARP 协议凭借 MAC 地址进行通信，IP 间的通信依赖 MAC地址。在网络上，通信的双方在同一局域网（LAN）内的情况是很少的，通常是经过多台计算机和网络设备中转才能连接到对方。而在进行中转时，会利用下一站中转设备的MAC地址来搜索下一个中转目标。这时，会采用 ARP 协议（AddressResolution Protocol）。ARP是一种用以解析地址的协议，根据通信方的 IP 地址就可以反查出对应的 MAC 地址
>
>- TCP传输层：提供可靠的字节流服务
>
>  所谓的字节流服务（Byte StreamService）是指，为了方便传输，将大块数据分割成以报文段（segment）为单位的数据包进行管理。而可靠的传输服务是指，能够把数据准确可靠地传给对方。一言以蔽之，TCP协议为了更容易传送大数据才把数据分割，而且 TCP 协议能够确认数据最终是否送达到对方
>
>  确保数据能到达目标，为了准确无误地将数据送达目标处，TCP 协议采用了三次握手（three-way handshaking）策略。用 TCP协议把数据包送出去后，TCP不会对传送后的情况置之不理，它一定会向对方确认是否成功送达。三次握手过程中使用了 TCP 的标志（flag） ——SYN（synchronize） 和ACK（acknowledgement）
>
>  - 发送端首先发送一个带 SYN 标志的数据包给对方。
>  - 接收端收到后，回传一个带有 SYN/ACK 标志的数据包以示传达确认信息。
>  - 最后，发送端再回传一个带 ACK 标志的数据包，代表“握手”结束。
>
>- DNS、HTTP应用层
>
>  DNS（Domain Name System）服务是和 HTTP 协议一样位于应用层的协议。它提供域名到 IP 地址之间的解析服务。
>
>  计算机既可以被赋予 IP 地址，也可以被赋予主机名和域名。比如h.badidu.com。
>
>  用户通常使用主机名或域名来访问对方的计算机，而不是直接通过 IP地址访问。因为与 IP 地址的一组纯数字相比，用字母配合数字的表示形式来指定计算机名更符合人类的记忆习惯。
>
>  但要让计算机去理解名称，相对而言就变得困难了。因为计算机更擅长处理一长串数字。
>
>  为了解决上述的问题，DNS 服务应运而生。DNS 协议提供通过域名查找 IP 地址，或逆向从 IP 地址反查域名的服务
>
>- 协议之间的关系
>
>  一次http请求：
>
>  ![image-20201116114915374](/Users/banggan/Library/Application Support/typora-user-images/image-20201116114915374.png)

##### TCP/UDP（传输层）

>- TCP三次握手及其必要性
>
>  建立连接前，客户端和服务端需要通过握手来确认对方:
>
>  SYN（synchronize） 和ACK（acknowledgement）
>
>  - 客户端发送 syn(同步序列编号) 请求，进入 syn_send 状态，等待确认
>  - 服务端接收并确认 syn 包后发送 syn+ack 包，以示传达确认信息，进入 syn_recv 状态
>  - 客户端接收 syn+ack 包后，发送 ack 包，双方进入 established 状态，代表“握手”结束\
>
>  必要性：双方都能明确自己和对方的收、发能力是正常的
>
>  - 第三次握手是为了防止失效的连接请求到达服务器，让服务器错误打开连接，因而产生错误
>
>    所谓已失效的连接请求报文段是这样产生的。A发送连接请求，但因连接请求报文丢失而未收到确认，于是A重发一次连接请求，成功后建立了连接。数据传输完毕后就释放了连接。现在假定A发出的第一个请求报文段并未丢失，而是在某个网络节点长时间滞留了，以致延误到连接释放以后的某个时间才到达B。本来这是一个早已失效的报文段。但B收到此失效的连接请求报文段后，就误以为A又发了一次新的连接请求，于是向A发出确认报文段，同意建立连接。假如不采用三次握手，那么只要B发出确认，新的连接就建立了。由于A并未发出建立连接的请求，因此不会理睬B的确认，也不会向B发送数据。但B却以为新的运输连接已经建立了，并一直等待A发来数据，因此白白浪费了许多资源。
>
>- TCP四次挥手
>
>  - 客户端发送一个数据包，告知服务端关闭数据传送，进入等待断开的状态
>
>  - 服务端收到后，返回一个ACK数据包，表示收到，
>
>  - 当服务端数据传输完毕后，向客户端发送一个数据包表示关闭数据传送;
>
>  - 客户端接受并发送ACK确认，断开TCP链接
>
>    - 客户端 -- FIN --> 服务端， FIN—WAIT
>    - 服务端 -- ACK --> 客户端， CLOSE-WAIT
>    - 服务端 -- ACK,FIN --> 客户端， LAST-ACK
>    - 客户端 -- ACK --> 服务端，CLOSED
>
>  - TCP四次挥手，是不是有一个wait_2的过程？time_wait？多长时间（2MSL）?为什么？
>
>    FIN_WAIT_2：应用程序端只收到服务器端得ACK信号，并没有收到FIN信号；说明服务器端还有数据传输，那么此时为半连接；
>
>    TIME_WAIT:有两种方式进入 该状态：
>
>    1. FIN_WAIT_1进入：此时应用程序端口收到FIN+ACK（而不是像FIN_WAIT_2那样只收到ACK，说明数据已经发送完毕）并 向服务器端口发送ACK；
>    2. FIN_WAIT_2进入：此时应用程序端口收到了FIN，然后向服务器端发送ACK；TIME_WAIT是为了实现TCP 全双工连接的可靠性关闭，用来重发可能丢失的ACK报文；需要持续2个MSL(最大报文生存时间)：假设应用程序端口在进入TIME_WAIT后，2个 MSL时间内并没有收到FIN,说明应用程序最后发出的ACK已经收到了；否则，会在2个MSL内在此收到ACK报文；
>
>    TIME_WAIT: 客户端接收到服务器端的 FIN 报文后进入此状态，此时并不是直接进入 CLOSED 状态，还需要等待一个时间计时器设置的时间才2MSL。这么做有两个理由：
>
>    - 确保最后一个确认报文能够到达。如果 B 没收到 A 发送来的确认报文，那么就会重新发送连接释放请求报文，A 等待一段时间就是为了处理这种情况的发生。如果没有等待时间，发送完确认报文段就立即释放连接的话，B就无法重传了（连接已被释放，任何数据都不能出传了），因而也就收不到确认，就无法按照步骤进入CLOSE状态，即必须收到确认才能close。
>
>    - 等待一段时间是为了让本连接持续时间内所产生的所有报文都从网络中消失，使得下一个新的连接不会出现旧的连接请求报文。
>    - 防止“已失效的连接请求报文段”出现在连接中。经过2MSL，那些在这个连接持续的时间内，产生的所有报文段就可以都从网络中消失。即在这个连接释放的过程中会有一些无效的报文段滞留在楼阁结点，但是呢，经过2MSL这些无效报文段就肯定可以发送到目的地，不会滞留在网络中。这样的话，在下一个连接中就不会出现上一个连接遗留下来的请求报文段了。
>    - 等待*2MSL*时间主要目的是怕最后一个*ACK*包对方没收到，那么对方在超时后将重发第三次握手的*FIN*包，主动关闭端接到重发的*FIN*包后可以再发一个*ACK*应答包。在*TIME_WAIT*状态时两端的端口不能使用，要等到*2MSL*时间结束才可继续使用。当连接处于*2MSL*等待阶段时任何迟到的报文段都将被丢弃。不过在实际应用中可以通过设置*SO_REUSEADDR*选项达到不必等待*2MSL*时间结束再使用此端口
>
>  - TCP 粘包、拆包说明
>
>    UDP 是基于报文发送的，UDP首部采用了 16bit 来指示 UDP 数据报文的长度，因此在应用层能很好的将不同的数据报文区分开，从而避免粘包和拆包的问题。
>
>    而 TCP 是基于字节流的，虽然应用层和 TCP 传输层之间的数据交互是大小不等的数据块，但是 TCP 并没有把这些数据块区分边界，仅仅是一连串没有结构的字节流；另外从 TCP 的帧结构也可以看出，在 TCP 的首部没有表示数据长度的字段，基于上面两点，在使用 TCP 传输数据时，才有粘包或者拆包现象发生的可能
>
>    - 什么是粘包、拆包？
>
>      假设 Client 向 Server 连续发送了两个数据包，用 packet1 和 packet2 来表示，那么服务端收到的数据可以分为三种情况，现列举如下
>
>      1. 接收端正常收到两个数据包，即没有发生拆包和粘包的现象。
>      2. 第二种情况，接收端只收到一个数据包，但是这一个数据包中包含了发送端发送的两个数据包的信息，这种现象即为粘包。这种情况由于接收端不知道这两个数据包的界限，所以对于接收端来说很难处理
>      3. 第三种情况，这种情况有两种表现形式，如下图。接收端收到了两个数据包，但是这两个数据包要么是不完整的，要么就是多出来一块，这种情况即发生了拆包和粘包。这两种情况如果不加特殊处理，对于接收端同样是不好处理的。
>
>    - 为什么TCP会出现粘包、拆包？
>
>      - 要发送的数据大于 TCP 发送缓冲区剩余空间大小，将会发生拆包。
>
>      - 待发送数据大于 MSS（最大报文长度），TCP 在传输前将进行拆包。
>
>      - 要发送的数据小于 TCP 发送缓冲区大小，TCP 将多次写入缓冲区的数据一次发送出去，将会发生粘包。
>
>      - 接收数据端的应用层没有及时读取接收缓冲区中的数据，将发生粘包。
>
>    - 粘包、拆包解决方法？
>
>      由于 TCP 本身是面向字节流的，无法理解上层的业务数据，所以在底层是无法保证数据包不被拆分和重组的，这个问题只能通过上层的应用协议栈设计来解决，根据业界的主流协议的解决方案，归纳如下：
>
>      - **消息定长：**发送端将每个数据包封装为固定长度（不够的可以通过补 0 填充），这样接收端每次接收缓冲区中读取固定长度的数据就自然而然的把每个数据包拆分开来。
>      - **设置消息边界：**服务端从网络流中按消息边界分离出消息内容。在包尾增加回车换行符进行分割，例如 FTP 协议。
>      - **将消息分为消息头和消息体：**消息头中包含表示消息总长度（或者消息体长度）的字段。
>      - 更复杂的应用层协议比如 Netty 中实现的一些协议都对粘包、拆包做了很好的处理
>
>  - TCP滑动窗口
>
>    窗口是缓存的一部分，用来暂时存放字节流。发送方和接收方各有一个窗口，接收方通过 TCP 报文段中的窗口字段告诉发送方自己的窗口大小，发送方根据这个值和其它信息设置自己的窗口大小。
>
>    发送窗口内的字节都允许被发送，接收窗口内的字节都允许被接收。如果发送窗口左部的字节已经发送并且收到了确认，那么就将发送窗口向右滑动一定距离，直到左部第一个字节不是已发送并且已确认的状态；接收窗口的滑动类似，接收窗口左部字节已经发送确认并交付主机，就向右滑动接收窗口。
>
>    接收窗口只会对窗口内最后一个按序到达的字节进行确认，例如接收窗口已经收到的字节为 {31, 34, 35}，其中 {31} 按序到达，而 {34, 35} 就不是，因此只对字节 31 进行确认。发送方得到一个字节的确认之后，就知道这个字节之前的所有字节都已经被接收
>
>    滑动窗口的基本原理：滑动窗口要比(接收、发送)缓存小。滑动窗口是针对于发送端和接收端的一种流量控制策略。在某些情况下，接收端处理数据的能力比发送端发送数据的能力低很多（或者发送端的数据太多），会造成接收端的队列塞满。因此有了滑动窗口，接收端告诉发送端一次最多可以发送多少数据
>
>    - TCP流量控制
>
>      流量控制是为了控制发送方发送速率，保证接收方来得及接收。
>
>      接收方发送的确认报文中的窗口字段可以用来控制发送方窗口大小，从而影响发送方的发送速率。将窗口字段设置为 0，则发送方不能发送数据。
>
>      **实际上，为了避免此问题的产生，发送端主机会时不时的发送一个叫做窗口探测的数据段**，此数据段仅包含一个字节来获取最新的窗口大小信息
>
>    - TCP使用滑动窗口做流量控制与乱序重排。保证TCP的可靠性与TCP的流控特性。
>
>      - 流量控制：
>
>      - - 滑动窗口的大小可以依据一定策略动态调整，应用会根据自身处理能力的变化，通过本端TCP接收窗口的大小的控制，实现对端的发送窗口改变进行流量控制。
>        - 接收方通过计算得出AdvertisedWindow，并发送给发送方。
>        - 发送方：根据AdvertisedWindow计算可发送最大的数据量EffectiveWindow。
>
>      - 乱序控制：
>
>      - - 接收方：按连续顺序确认接收并发送ack信号。
>        - 发送方：按连续顺序发送数据。
>
>    - 窗口更新的过程
>
>      1. 接收方的处理数据能力 > 缓冲区的增长速度，这个时候接收方缓冲区的空间就会增大，因此 Window size 也会增大。
>      2. 发送方接受到接收方发来的ack，窗口的左边界就会向右移动，同时也会根据 Window size 调整窗口的右边界。
>
>      特殊情况：当头部的 Window size 是 0 的时候，发送方就不能继续发送数据了。那当我的接收方处理掉一部分数据之后，发送方怎么知道接收方有新的空间？当 Window size 为 0 的时候，发送方会发送一个包 ZWP(Zero Window Probe) 给接收方，接收方收到包以后，会以 ack 的形式返回目前的 Window size。如果发了三个 ZWP，返回的都是 0， 那么发送方会选择异常关闭连接(RST)。异常关闭连接(RST)不用等缓冲区的数据都发送完毕，接收方也不需要发送 ack。
>
>      总的看下来，滑动窗口是一个在端对端的流量控制策略。 一开始两端会协商一个初始的窗口大小，然后根据窗口大小来确定发送哪些数据；数据传输过程中，接收端不断更新 Window size， 发送方根据这个返回的值更新自己的窗口大小，达到一种流量控制的目的
>
>  - TCP拥塞控制
>
>    TCP 主要通过四个算法来进行拥塞控制：**慢开始、拥塞避免、快速重传、快速恢复**
>
>    如果网络出现拥塞，分组将会丢失，此时发送方会继续重传，从而导致网络拥塞程度更高。因此当出现拥塞时，应当控制发送方的速率。这一点和流量控制很像，但是出发点不同。流量控制是为了让接收方能来得及接收，而拥塞控制是为了降低整个网络的拥塞程度
>
>    发送方需要维护一个叫做拥塞窗口（cwnd）的状态变量，注意拥塞窗口与发送方窗口的区别：拥塞窗口只是一个状态变量，实际决定发送方能发送多少数据的是发送方窗口
>
>    - 慢启动和拥塞避免
>
>      发送的最初执行慢开始，令cwnd = 1，发送方只能发送 1 个报文段；当收到确认后，将 cwnd 加倍，因此之后发送方能够发送的报文段数量为：2、4、8 ...
>
>      注意到慢开始每个轮次都将 cwnd 加倍，这样会让 cwnd 增长速度非常快，从而使得发送方发送的速度增长速度过快，网络拥塞的可能性也就更高。设置一个慢开始门限 ssthresh，当 cwnd >= ssthresh 时，进入拥塞避免，每个轮次只将cwnd 加 1。如果出现了超时，则令 ssthresh = cwnd / 2，然后重新执行慢开始
>
>    - 快速重传和快速恢复
>
>      在接收方，要求每次接收到报文段都应该对最后一个已收到的有序报文段进行确认。例如已经接收到 M1 和 M2，此时收到 M4，应当发送对 M2 的确认。
>
>      在发送方，如果收到三个重复确认，那么可以知道下一个报文段丢失，此时执行快重传，立即重传下一个报文段。例如收到三个 M2，则 M3 丢失，立即重传 M3。
>
>      在这种情况下，只是丢失个别报文段，而不是网络拥塞。因此执行快恢复，令 ssthresh = cwnd / 2 ，cwnd = ssthresh，注意到此时直接进入拥塞避免。慢开始和快恢复的快慢指的是 cwnd 的设定值，而不是 cwnd 的增长速率。慢开始 cwnd 设定为 1，而快恢复 cwnd 设定为 ssthresh
>
>  - TCP可靠传输
>
>    TCP 的超时重传来实现可靠传输：如果一个已发送的报文段在超时时间内没有收到确认，那么就重传这个报文段。
>
>    一个报文段从发送再到接收到确认所经过的时间称为往返时间 RTT，加权平均往返时间 RTTs 计算如下：
>
>    ![image](https://cdn.nlark.com/yuque/0/2020/png/420158/1598769743242-4158eb2d-e8b8-4403-b400-bc59154b10d0.png)
>
>    其中，0 ≤ a ＜ 1，RTTs 随着 a 的增加更容易受到 RTT 的影响。超时时间 RTO 应该略大于 RTTs，TCP 使用的超时时间计算如下：
>
>    ![image](https://cdn.nlark.com/yuque/0/2020/png/420158/1598769743261-dab71365-6174-4e50-956e-297cc2db8f27.png)
>
>    其中 RTTd 为偏差的加权平均值
>
>  - TCP的端链接和长链接
>
>    **短连接：**Client 向 Server 发送消息，Server 回应 Client，然后一次读写就完成了，这时候双方任何一个都可以发起 close 操作，不过一般都是 Client 先发起 close 操作。短连接一般只会在 Client/Server 间传递一次读写操作。
>
>    短连接的优点：管理起来比较简单，建立存在的连接都是有用的连接，不需要额外的控制手段。
>
>    **长连接：**Client 与 Server 完成一次读写之后，它们之间的连接并不会主动关闭，后续的读写操作会继续使用这个连接。
>
>    在长连接的应用场景下，Client 端一般不会主动关闭它们之间的连接，Client 与 Server 之间的连接如果一直不关闭的话，随着客户端连接越来越多，Server 压力也越来越大，这时候 Server 端需要采取一些策略，如关闭一些长时间没有读写事件发生的连接，这样可以避免一些恶意连接导致 Server 端服务受损；如果条件再允许可以以客户端为颗粒度，限制每个客户端的最大长连接数，从而避免某个客户端连累后端的服务。
>
>    长连接和短连接的产生在于 Client 和 Server 采取的关闭策略，具体的应用场景采用具体的策略
>
>  - TCP、UDP的区别？优劣点？使用场景？
>
>    **传输控制协议（Transmission Control Protocol）**
>
>    TCP 是**面向连接**的、**可靠**的流协议。流就是指不间断的数据结构，当应用程序采用 TCP 发送消息时，虽然可以保证发送的顺序，但还是犹如没有任何间隔的数据流发送给接收端
>
>    - 是面向连接的，提供可靠交付，有流量控制，拥塞控制，提供全双工通信
>
>    - - TCP 是一种面向连接的协议，只有在确认通信对端存在时才会发送数据，从而可控制通信流量的浪费。
>      - 全双工，用滑动窗口来进行流量控制；
>      - 拥塞控制：慢开始，拥塞避免，快重传，快恢复；
>      - TCP会发生 粘包、拆包： TCP 是基于字节流的，虽然应用层和 TCP 传输层之间的数据交互是大小不等的数据块，但是 TCP 并没有把这些数据块区分边界，仅仅是一连串没有结构的字节流；另外从 TCP 的帧结构也可以看出，在 TCP 的首部没有表示数据长度的字段，基于上面两点，在使用 TCP 传输数据时，才有粘包或者拆包现象发生的可能。
>
>    - 面向字节流（把应用层传下来的报文看成字节流，把字节流组织成大小不等的数据块），每一条 TCP 连接只能是点对点的（一对一）。
>    - TCP充分地实现了数据传输时各种控制功能，可以进行丢包时的重发控制，还可以对次序乱掉的分包进行顺序控制。而这些在 UDP 中都没有。
>    - 应用场景：对效率要求低，对准确性要求高或者要求有连接的场景
>
>    **用户数据报协议 UDP（User Datagram Protocol）**
>
>    UDP 是**面向报文**的，无论应用层交给UDP多长的报文，UDP就照样发送，即一次发送一个报文。因此，应用程序必须选择合适大小的报文。若报文太长，则IP层需要分片，降低效率。若太短，会是IP太小。
>
>    UDP 是**不具有可靠性**的数据报协议，细微的处理它会交给上层的应用去完成。在 UDP 的情况下，虽然可以确保发送消息的大小，却不能保证消息一定会到达。因此，应用有时会根据自己的需要进行重发处理
>
>    - 是无连接的，尽最大可能交付，没有拥塞控制
>
>    - - 将应用程序发来的数据在收到的那一刻，立即按照原样发送到网络上的一种机制。
>      - UDP不会发生 粘包、拆包： UDP 是基于报文发送的，UDP首部采用了 16bit 来指示 UDP 数据报文的长度，因此在应用层能很好的将不同的数据报文区分开，从而避免粘包和拆包的问题。
>
>    - 面向报文（对于应用程序传下来的报文不合并也不拆分，只是添加 UDP 首部），支持一对一、一对多、多对一和多对多的交互通信。
>    - 传输途中出现丢包，UDP 也不负责重发。当包的到达顺序出现乱序时，UDP没有纠正的功能。
>    - 应用场景：对效率要求高，对准确性要求低的场景。-- 对高速传输和实时性有较高要求的通信或广播通信
>
>    1. 1. 包总量较少的通信（DNS、SNMP等）；
>       2. 视频、音频等多媒体通信（即时通信）；
>       3. 限定于 LAN 等特定网络中的应用通信；
>       4. 广播通信（广播、多播
>
>    **总结**：TCP 和 UDP 的优缺点无法简单地、绝对地去做比较：TCP 用于在传输层有必要实现可靠传输的情况； 而在一方面，UDP 主要用于那些对高速传输和实时性有较高要求的通信或广播通信。TCP 和 UDP 应该根据应用的目的按需使用
>
>  - TCP、UDP传输需要几个数据包？
>
>    1500字节被称为链路层的MTU(最大传输单元).1500-20-8=1472
>
>    在普通的局域网环境下，建议将UDP的数据控制在1472字节以下为好
>
>    还有地方说还应该有个PPP的包头包尾的开销（8Bytes),那就为1492了
>
>    - UDP 包的大小就应该是 1492 - IP头(20) - UDP头(8) = 1464(BYTES)
>    - TCP 包的大小就应该是 1492 - IP头(20) - TCP头(20) = 1452(BYTES)
>
>    鉴于Internet上的标准MTU值为576字节,所以我建议在进行Internet的UDP编程时.最好将UDP的数据长度控件在548字节(576-8-20)以内.（8为UDP头。20为IP头）
>
>  - TCP、UDP首部格式？
>
>    - TCP 首部格式比 UDP 复杂：（20字节的固定首部）
>
>      ![image-20201116193005957](/Users/banggan/Library/Application Support/typora-user-images/image-20201116193005957.png)
>
>      - 序号：对字节流进行编码，例如序号为 301，表示第一个字节的编号为 301，如果携带的数据长度为 100 字节，那么下一个报文段的序号应为 401。
>      - 确认号：期望收到的下一个保文段的序号；例如 B 正确收到 A 发送来的一个报文段，序号为 501，携带的数据长度为 200 字节，因此 B 期望下一个报文段的序号为 701，B 发送给 A 的确认报文段中确认号就为 701
>      - 数据偏移：数据部分距离报文段起始处的偏移量，其实就是指首部的长度；
>      - 控制位：八位从左到右分别是 CWR，ECE，URG，ACK，PSH，RST，SYN，FIN
>        - **CWR**:CWR 标志与后面的 ECE 标志都用于 IP 首部的 ECN 字段，ECE 标志为 1 时，则通知对方已将拥塞窗口缩小；
>        - **ECE：**若其值为 1 则会通知对方，从对方到这边的网络有阻塞。在收到数据包的 IP 首部中 ECN 为 1 时将 TCP 首部中的 ECE 设为 1；
>        - **URG：**该位设为 1，表示包中有需要紧急处理的数据，对于需要紧急处理的数据，与后面的紧急指针有关；
>        - **ACK：**该位设为 1，确认应答的字段有效，TCP规定除了最初建立连接时的 SYN 包之外该位必须设为 1；
>        - **PSH：**该位设为 1，表示需要将收到的数据立刻传给上层应用协议，若设为 0，则先将数据进行缓存；
>        - **RST：**该位设为 1，表示 TCP 连接出现异常必须强制断开连接；
>        - **SYN：**用于建立连接，该位设为 1，表示希望建立连接，并在其序列号的字段进行序列号初值设定；
>        - **FIN：**该位设为 1，表示今后不再有数据发送，希望断开连接。当通信结束希望断开连接时，通信双方的主机之间就可以相互交换 FIN 位置为 1 的 TCP 段
>
>    - UDP 首部字段只有8 个字节，包括源端口、目的端口、长度、检验和。12 字节的伪首部是为了计算检验和临时添加的

##### HTTP（应用层）

>- http1.1与http1.0
>
>  **1.0 协议缺陷:**
>
>  1. **无法复用链接**，完成即断开，重新慢启动和 TCP 3次握手
>  2. head of line blocking: **线头阻塞**，导致请求之间互相影响
>
>  **1.1 改进:**
>
>  - **长连接**(默认 keep-alive)，复用
>  - host 字段指定对应的虚拟站点
>  - 新增功能:
>
>  - - 断点续传、身份认证、状态管理
>    - cache 缓存：Cache-Control/Expires，Last-Modified，Etag
>
>  **http1.1相对http1.0优化：**
>
>  1. 长连接：复用链接；数据传输完成了保持TCP连接不断开；Connection：Keep-Alive头域 ; 
>  2. 管线化技术：管线化技术是在持久化连接的基础上，进一步对通信性能的提升。在持久化连接下，请求是顺次进行的。上次请求得到响应后，才能发送下次请求。管线化技术就是指能在未收到响应时，顺次发送多个响应。
>  3. 增加host头域：请求消息和响应消息都支持Host头域，且请求消息中没有头域会报告一个错误（400 Bad Request），服务器应该接受以绝对路径标记的资源请求。
>  4. 引入transfer-Encoding：chunked将发送方消息分割成若干个任意大小的数据块，每个数据块在发送时都附上块长度，最后用一个零长度的块作为消息结束标志；允许发送方只缓存消息的一个片段，避免缓存整个消息带来的过载。
>  5. 缓存处理：在HTTP1.0主要使用Expire判断强缓存；1.1之后引入Cache-Control（请求和响应消息都可以使用），支持一个可扩展的指令集
>
>- Http2.0 和 http1.1
>
>  **http1.1(1997年发布)的缺陷：（安全不足，性能不高）**
>
>  1. 高延迟--带来页面加载速度的降低
>  2. 无状态特性 -- 带来巨大的HTTP头部
>  3. 明文传输 -- 带来的不安全性
>  4. 不支持服务器推送消息
>
>  提高性能的措施：会引入雪碧图、将小图内联、使用多个域名等等的方法
>
>  **http2(2015年)相对http1.1优化**:
>
>  1. **二进制传输/** **二进制分帧**(应用层和传输层之间)
>
>     HTTP/2传输数据量的大幅减少,主要有两个原因:以二进制方式传输和Header 压缩。
>
>     HTTP/2 将请求和响应数据分割为更小的帧，并且它们采用二进制编码。
>
>     HTTP/2在应用层和传输层之间追加了一个二进制分帧层，最终使得多个数据流共用一个连接，更加高效的使用tcp连接。从而使得服务器的连接压力减轻，降低了内存的消耗，增大了网络的吞吐量。
>
>  2. **报文头Header压缩：**HTTP/2引入了HPACK算法对头部进行压缩，大大减小了数据发送的字节数。减小HTTP报文中头部字段的开销，提供通信效率；报文头压缩算法优化：基于静态字典压缩；基于动态字典压缩
>
>  3. **多路复用：**允许同时通过单一的 HTTP/2 连接发起多重的请求-响应消息。
>
>  - - 实现多流并行而不用依赖建立多个 TCP 连接 => 站点会有多个静态资源 CDN 域名的原因之一；
>    - 建立在持久连接基础上，允许多个请求公用同一连接，并且能够并行传输。管线化技术中，所有请求是顺次发送出去的；而多路复用中，所有请求是并行发送出去的。
>
>  4. **服务端推送 （**Server Push**）**
>
>  5. **提高安全性:** 出于兼容的考虑，HTTP/2延续了HTTP/1的 明文传输数据 特点，但互联网上通常所能见到的HTTP/2都是使用"https”协议名，跑在TLS上面，所以“事实上”的HTTP/2是加密的
>
>- http3 和 http2
>
>  **http2的缺点**：(主要是底层支撑的 TCP 协议造成的)
>
>  - TCP 以及 TCP+TLS建立连接的延时   
>  - TCP的队头阻塞并没有彻底解决
>
>  - - 在HTTP/2中，多个请求是跑在一个TCP管道中的。但当出现丢包时，HTTP/2 的表现反倒不如 HTTP/1 。
>    - 因TCP为了保证可靠传输，有个特别的“丢包重传”机制，丢失的包必须要等待重新传输确认。HTTP/2出现丢包时，整个 TCP 都要开始等待重传，那么就会阻塞该TCP连接中的所有请求。而对于HTTP/1.1来说，可开启多个 TCP 连接，出现这种情况反到只影响其中一个连接，剩余的TCP连接还可正常传输数据。
>
>  **http3**:HTTP/3基于 UDP 协议的“QUIC”协议，让HTTP跑在QUIC上而不是TCP上。 它在HTTP/2的基础上又实现了质的飞跃，真正“完美”地解决了“队头阻塞”问题
>
>   (快速握手、零RTT、类似TCP的流量控制、传输可靠性的功能、集成了TLS加密功能、多路复用-彻底解决队头阻塞)
>
>  **QUIC新功能：**
>
>  QUIC基于UDP，而UDP是“无连接”的，根本就不需要“握手”和“挥手”，所以就比TCP来得快。
>
>  - 实现了类似TCP的流量控制、传输可靠性的功能。提供了 数据包重传、拥塞控制以及一些TCP中存在的特性。
>  - 实现了快速握手功能。
>  - 集成了TLS加密功能。
>  - 多路复用，彻底解决TCP中队头阻塞的问题
>
>  - - 和TCP不同，QUIC实现了在同一物理连接上可以有多个独立的逻辑数据流。实现了数据流的单独传输，就解决了TCP中队头阻塞的问题
>
>- http 和 https的区别
>
>  **http存在的问题**：
>
>  ​	请求信息明文传输，容易被窃听截取。
>
>  ​	数据的完整性未校验，容易被篡改
>
>  ​	没有验证对方身份，存在冒充危险
>
>  **https**
>
>  HTTPS 协议（HyperText Transfer Protocol over Secure Socket Layer）：一般理解为HTTP+SSL/TLS，通过 SSL证书来验证服务器的身份，并为浏览器和服务器之间的通信进行加密
>
>  - 那么SSL/TLS又是什么?
>
>    - TLS（传输层安全）跟SSL（安全套接字）可以理解成类似的东西，可以将SSL理解成负责对HTTP的数据进行加密的加密套件，而TLS是SSL的升级版/继任者。
>    - SSL（Secure Socket Layer，安全套接字层）：1994年为 Netscape 所研发，SSL 协议位于 TCP/IP 协议与各种应用层协议之间，为数据通讯提供安全支持。
>    - TLS（Transport Layer Security，传输层安全）：其前身是 SSL，它最初的几个版本（SSL 1.0、SSL 2.0、SSL 3.0）由网景公司开发，1999年从 3.1 开始被 IETF 标准化并改名，发展至今已经有 TLS 1.0、TLS 1.1、TLS 1.2 三个版本。SSL3.0和TLS1.0由于存在安全漏洞，已经很少被使用到。TLS 1.3 改动会比较大，目前还在草案阶段，目前使用最广泛的是TLS 1.1、TLS 1.2
>
>  - https证书如何效验?
>
>    - 浏览器从服务器拿到证书。证书上有服务器的公钥和CA机构打上的数字签名。
>    - 拿到证书后验证其数字签名。具体就是，根据证书上写的CA签发机构，在浏览器内置的根证书里找到对应的公钥，用此公钥解开数字签名，得到摘要（digest,证书内容的hash值），据此验证证书的合法性
>
>    - 
>
>  - https接下来的加密过程?
>
>    - 验证完ca证书的合法性后，在证书里取出服务器的公钥。
>    - 浏览器生成**对称密钥**。
>    - 浏览器使用服务器公钥对该**对称密钥**加密，发回给服务器。
>    - 服务器使用服务器私钥解密，得到**对称密钥。**
>    - 服务器使用该对称密钥加密后续http数据。使用对称密钥加密是因为比非对称加密高效
>
>  - https 完整的建立过程
>
>    1. 客户端向服务器请求https连接，服务端返回CA证书（公钥）浏览器验证完证书的合法性后，取出证书中的服务器公钥
>    2. 浏览器生成随机对称密钥
>    3. 浏览器使用服务器公钥对该**对称密钥**加密，发回给服务器   (公钥加密，私钥解密 为非对称加密)
>    4. 服务器使用服务器私钥解密，得到**对称密钥。**    (公钥加密--私钥解密 为非对称加密)
>    5. 服务器使用该对称密钥加密后续http数据：使用对称密钥加密是因为比非对称加密高效
>
>  - Https 的优点
>
>    - 使用HTTPS协议可认证用户和服务器，确保数据发送到正确的客户机和服务器；
>    - HTTPS协议是由SSL+HTTP协议构建的可进行加密传输、身份认证的网络协议，要比http协议安全，可防止数据在传输过程中不被窃取、改变，确保数据的完整性；
>    - HTTPS是现行架构下最安全的解决方案，虽然不是绝对安全，但它大幅增加了中间人攻击的成本
>
>  - https 的缺点
>
>    - HTTPS协议多次握手，导致页面的加载时间延长近50%；
>    - HTTPS连接缓存不如HTTP高效，会增加数据开销和功耗；
>    - 申请SSL证书需要钱，功能越强大的证书费用越高。
>    - SSL涉及到的安全算法会消耗 CPU 资源，对服务器资源消耗较大
>
>  - Https 和 http 的使用场景
>
>    - HTTP协议的数据传输是明文的，是不安全的；因此HTTP协议不适合传输一些敏感/隐私信息，比如：账号、密码等信息。
>    - https应用于任何场景！
>
>  - 总结 https 和 http 的区别
>
>    - 安全性不同：HTTPS是HTTP协议的安全版本。HTTP协议 是明文传输协议，是不安全的；HTTPS 协议是由 SSL/TLS+HTTP 协议构建的可进行加密传输、身份认证的网络协议，比 HTTP 协议安全。
>    - 默认端口不同：HTTPS标准端口443，HTTP标准端口80;
>    - SEO：HTTPS对搜索引擎更友好，利于SEO,谷歌、百度优先索引HTTPS网页;
>    - SSL证书：HTTPS需要申请SSL证书，而HTTP不用;  => 由CA认证机构发放
>    - HTTPS基于传输层，HTTP基于应用层; http和https使用连接方式不同
>    - HTTPS在浏览器显示绿色安全锁，HTTP没有显示
>
>- TLS(安全传输层协议)
>
>  - 对称加密和非对称加密的区别是什么？ 举例几个常用的加密算法？
>
>    - 对称加密：私钥加密：发送者A用B的私钥加密，接收者B用B的私钥解密。
>
>      对称加密是最快速、最简单的一种加密方式，加密（encryption）与解密（decryption）用的是同样的密钥（secret key）,这种方法在密码学中叫做对称加密算法。对称加密有很多种算法，由于它效率很高，所以被广泛使用在很多加密协议的核心当中。对称加密通常使用的是相对较小的密钥，一般小于256 bit。因为密钥越大，加密越强，但加密与解密的过程越慢。如果你只用1 bit来做这个密钥，那黑客们可以先试着用0来解密，不行的话就再用1解；但如果你的密钥有1 MB大，黑客们可能永远也无法破解，但加密和解密的过程要花费很长的时间。密钥的大小既要照顾到安全性，也要照顾到效率，是一个trade-off
>
>    - 非对称加密：公钥加密：发送者A用B的公钥加密，接受者B用B的私钥解密。
>
>      非对称加密为数据的加密与解密提供了一个非常安全的方法，它使用了一对密钥，公钥（public key）和私钥（private key）。私钥只能由一方安全保管，不能外泄，而公钥则可以发给任何请求它的人。非对称加密使用这对密钥中的一个进行加密，而解密则需要另一个密钥。比如，你向银行请求公钥，银行将公钥发给你，你使用公钥对消息加密，那么只有私钥的持有人--银行才能对你的消息解密。与对称加密不同的是，银行不需要将私钥通过网络发送出去，因此安全性大大提高
>
>    - 两种方式的使用
>
>      - 对称密钥加密 ，它是信息的发送方和接收方都用同一个秘钥去加密和解密数据。这样做它的最大优势是加/解密速度快，适合于对大数据量进行密，但密钥管理困难。
>
>      - 非对称密钥加密，它需要使用“一对”密钥来分别完成加密和解密操作，一个公开发布，即公开密钥，另一个由用户自己秘密保存，即私用密钥。信息发送者用公开密钥去加密，而信息接收者则用私用密钥去解密。公钥机制灵活，但加密和解密速度却比对称密钥加密慢得多
>
>    - 非对称加密的使用过程
>
>      1. A要向B发送信息，A和B都要产生一对用于加密和解密的公钥和私钥。
>      2. A的私钥保密，A的公钥告诉B；B的私钥保密，B的公钥告诉A。
>      3. A要给B发送信息时，A用B的公钥加密信息，因为A知道B的公钥。
>      4. 将这个消息发给B（已经用B的公钥加密消息）。
>      5.  B收到这个消息后，B用自己的私钥解密A的消息，其他所有收到这个报文的人都无法解密，因为只有B才有B的私钥。
>      6. 反过来，B向A发送消息也是一样
>
>    - 非对称加密和对称加密的区别
>
>      - 对称加密加密与解密使用的是同样的密钥，所以速度快。但由于需将密钥在网络传输，所以安全性不高
>      - 非对称加密使用了一对密钥，公钥与私钥，所以安全性高，但加密与解密速度慢。
>      - 解决的办法是将对称加密的密钥使用非对称加密的公钥进行加密，然后发送出去，接收方使用私钥进行解密得到对称加密的密钥，然后双方可以使用对称加密来进行沟通。 => 混合加密
>
>    - 常见的加密算法
>
>      MD5、SHA-1、SHA-2、AES、DES、Base64
>
>- 请求方式：GET、POST协议的区别 限制
>
>  - get post的区别
>    - get 请求数据可以缓存、可收藏为书签、参数会保留在浏览器历史记录；post请求数据不可缓存，不可收藏为书签、参数不会保存在浏览器历史中。
>    - 都包含请求头请求行，但post多了请求body。
>    - 使用方式不同，安全性不同：GET直接添加到URL后面的，可直接在URL中看到内容。而POST是放在报文内部的，用户无法直接看到，所以更安全。
>    - 长度限制不同：GET提交的数据长度是有限制的，因为URL长度有限制，具体的长度限制视浏览器而定(请求长度受限最多1024kb)。而POST数据长度无限制、无数据类型限制。
>    - 作用不同：GET多用来查询，请求参数放在url中，不会对服务器上的内容产生作用。POST用来提交，如把账号密码放入body中，更安全。
>    - GET请求产生一个TCP数据报；POST产生两个。
>    - 对于GET浏览器会把http header和data一并发送出去，服务器响应200（返回数据）；而POST浏览器先发送header，服务器响应100 continue，浏览器再发送data，服务器响应200（返回数据）Firefox只发一次
>
>- http 报文：请求和响应头
>
>  - http报文的请求会有几个部分？
>
>    一个HTTP请求报文由请求行（request line）、请求头（header）、空行和请求数据4个部分组成
>
>    - 请求报文： 请求行、请求头、请求正文
>    - 响应报文： 状态行、响应头、响应正文
>
>  - 常见的请求头、响应头
>
>    - 请求头
>
>      | 常见请求头                                      | 意义                       |
>      | ----------------------------------------------- | -------------------------- |
>      | Accept: text/html; image/webp;                  | 浏览器可接收的类型         |
>      | Accept-Encoding: gzip,compress                  | 浏览器可接收的压缩编码类型 |
>      | If-Modified-Since: Mon, 1 Aug 2016 18:23:51 GMT | 页面缓存时间               |
>      | Cache-Control：max-age=300                      | 缓存的最长时间 300s        |
>
>    - 响应头
>
>      | 常见响应头                                  | 意义                                                       |
>      | ------------------------------------------- | ---------------------------------------------------------- |
>      | Content-Encoding: gzip                      | 服务器发送的压缩编码方式                                   |
>      | Content-Length: 80                          | 服务器发送显示的字节码长度                                 |
>      | Content-Type: text/html; charset=UTF-8      | 服务器发送内容的类型和编码类型                             |
>      | Last-Modified: Tue, 11 Jul 2000 18:23:51GMT | 服务器最后一次修改的时间                                   |
>      | Date:Mon, 01 Aug 2016 13:38:43 GMT          | 资源起始时间                                               |
>      | Expires:Mon, 01 Aug 2016 13:38:53 GMT       | 资源过期时间                                               |
>      | ETag:W/”578303987844e7989822712c1e153fc9”   | 资源实体的标识(唯一标识，类似md5值，文件有修改md5就不一样) |
>      | Cache-Control: max-age=10                   | 缓存时间                                                   |
>
>  - 缓存相关的http头部
>
>    - Expires / Cache-Control
>
>      应当如何确定缓存文件的有效时间呢？对此，我们需要用到HTTP头中的Expires和Cache-Control。
>
>      - Expires: 当客户端第一次访问一个文件资源的时候，服务端在返回资源内容的同时也返回了
>
>      - Cache-control: 
>
>        由于Expires给定的是绝对时间，而客户端的系统时间可以由用户任意修改，比如Expires设定的过期时间是：`Mon, 1 Aug 2016 22:43:02 GMT`而现在用户吧系统时间改为`Tue, 2 Aug 2016 22:43:02 GMT`
>
>        则缓存会被判为过期（虽然实际上还没到那个时间）。因此在HTTP1.1中引入了Cache-Control，这就是一个相对时间，比如`Cache-Control: max-age=80`那就是说这份缓存的有效期是80秒，而没有给定过期的绝对时间。
>
>        由于Cache-Control是HTTP1.1中才有的，因此可能会有Expires和Cache-Control同时出现的情况，这时以Cache-Control为准。
>
>    - Last-Modified / If-Modified-Since
>
>      现在有另外一个问题，服务端有个文件可能会更新，因此希望客户端时不时过来问一下这个文件是否过期。
>
>      如果没有过期，服务端不返回数据给浏览器，只返回304状态码，告诉浏览器目前的缓存还没有过期，然后浏览器继续使用已有缓存,这个就叫做条件请求。这里就要用到以下两个头部信息
>
>      - Last-Modified (response header)
>      - If-Modified-Since (request header)
>
>      以上一小节的响应头和请求头为例，浏览器第一次请求资源的时候，服务端返回资源内容，同时也返回了`Last-Modified:Mon, 01 Aug 2016 13:48:44 GMT`也就是服务端在告诉客户端这个文件在服务器上的最后修改时间。
>
>      浏览器第二次访问的时候（假设这里没有设置Expires或者Cache-Control）。那么浏览器在访问资源的时候会在请求头上带上`If-Modified-Since:Mon, 01 Aug 2016 13:48:44 GMT`
>
>      服务端收到后对比目前文件的最后修改时间和该请求头的信息，如果没有修改，那就直接返回304给浏览器，而不返回实际资源。如果有变化了，就返回200，并且带上新的资源内容
>
>    - Etag / If-None-Match
>
>      条件请求还有另外一种方法——打标签，也就是使用Etag。
>
>      - 第一次拿到资源的时候，服务器的响应头中包含了Etag，用来作为时间标签
>      - 下一次浏览器再次请求资源的时候会把原来的Etag标签带上（在请求头中变成了If-None-Match）作为校验标准，
>      - 若这个文件如果发生了改变，则Etag也会改变。
>      - 服务器对比浏览器请求头中的的If-None-Match：
>
>      - - 如果相同就返回304，而不返回实际资源
>        - 如果不同，就返回200和新的资源。
>
>      由于Etag需要通过服务器计算得出，每次都进行计算需要额外的开销，有时候这也是一种负担
>
>- 响应状态码
>
>  访问一个网页时，浏览器会向web服务器发出请求。此网页所在的服务器会返回一个包含HTTP状态码的信息头用以响应浏览器的请求。
>
>  **状态码分类**：
>
>  - 1XX- 信息型，服务器收到请求，需要请求者继续操作。
>  - 2XX- 成功型，请求成功收到，理解并处理。
>  - 3XX - 重定向，需要进一步的操作以完成请求。
>  - 4XX - 客户端错误，请求包含语法错误或无法完成请求。
>  - 5XX - 服务器错误，服务器在处理请求的过程中发生了错误。
>
>  **常见状态码**：
>
>  **2xx** **请求成功**
>
>  - 200 OK：表示从客户端发送给服务器的请求被正常处理并返回；
>  - 204 No Content：表示客户端发送给客户端的请求得到了成功处理，但在返回的响应报文中不含实体的主体部分（没有资源可以返回）；
>  - 207 由WebDAV(RFC 2518)扩展的状态码，代表之后的消息体将是一个XML消息，并且可能依照之前子请求数量的不同，包含一系列独立的响应代码。
>
>  **3xx** **重定向相关**
>
>  - 301 Moved Permanently： 永久性重定向，表示请求的资源被分配了新的URL，之后应使用更改的URL；
>  - 302 Found：临时性重定向，表示请求的资源被分配了新的URL，希望本次访问使用新的URL；
>
>  301与302的区别：前者是永久移动，后者是临时移动（之后可能还会更改URL）
>
>  - 304 Not Modified：（未修改） 自从上次请求后，请求的网页未修改过。 服务器返回此响应时，不会返回网页内容。资源已经找到，但是不满足条件，所以不把资源返回给客户端。常用于协商缓存。
>
>  **4xx** **客户端错误相关，或无法完成请求**
>
>  - 400 Bad Request - 客户端请求有语法错误，不能被服务器所理解
>  - 401 Unauthorized：请求未经授权，需要通过HTTP认证；这个状态代码必须和WWW-Authenticate报头域一起使用
>  - 403 Forbidden：（禁止）服务器拒绝该次访问（访问权限出现问题）
>  - 404 Not Found：表示服务器上无法找到请求的资源，除此之外，也可以在服务器拒绝请求但不想给拒绝原因时使用； (请求资源不存在，可能是输入了错误的URL)
>  - 405（方法禁用） 禁用请求中指定的方法
>
>  **5xx**（服务器错误）这些状态代码表示服务器在尝试处理请求时发生内部错误。这些错误可能是服务器本身的错误，而不是请求出错。
>
>  - 500 Inter Server Error：（服务器内部错误） 表示服务器在执行请求时发生了错误，也有可能是web应用存在的bug或某些临时的错误时；
>  - 502（错误网关） 服务器作为网关或代理，从上游服务器收到无效响应。
>  - 503 Server Unavailable: (服务不可用)表示服务器暂时处于超负载或正在进行停机维护，无法处理客户端的请求；
>  - 504 （网关超时） 服务器作为网关或代理，但是没有及时从上游服务器收到请求
>
>- http无状态协议的理解
>
>  - 无状态协议对于事务处理没有记忆能力。缺少状态意味着如果后续处理需要前面的信息，则必须重传。
>
>  - - 也就是说，当客户端一次HTTP请求完成以后，客户端再发送一次HTTP请求，HTTP并不知道当前客户端是一个”老用户“。
>
>  - 可以使用Cookie来解决无状态的问题，Cookie就相当于一个通行证，第一次访问的时候给客户端发送一个Cookie，当客户端再次来的时候，拿着Cookie(通行证)，那么服务器就知道这个是”老用户“

##### 浏览器缓存

>- DNS缓存？DNS负载均衡
>
>  - 什么是DNS？：全称 `Domain Name System`域名解析系统；作用：根据域名查出IP地址。
>
>  - DNS解析： 简单的说，通过域名最终解析到该域名对应的IP地址`www.dnscache.com (域名)  - DNS解析 -> 11.222.33.444 (IP地址)`
>
>  - DNS缓存:有DNS的地方,就有缓存! 浏览器、操作系统、Local DNS、根域名服务器，都会对DNS结果做一定程度的缓存。
>
>  - DNS查询过程
>
>    浏览器缓存->系统缓存->hosts->DNS。
>
>    - 首先搜索浏览器自身的DNS缓存,如果存在，则域名解析到此完成。
>    - 如果浏览器自身的缓存里面没有找到对应的条目，那么会尝试读取操作系统的hosts文件看是否存在对应的映射关系,如果存在，则域名解析到此完成。hosts文件是一个操作系统文件，以表的形式存储了主机名 和 IP地址，用于查找主机名称。
>    - 如果本地hosts文件不存在映射关系，则查找本地DNS服务器(ISP服务器,或者自己手动设置的DNS服务器),如果存在,域名到此解析完成。
>    - 如果本地DNS服务器还没找到的话,它就会向根服务器发出请求,进行递归查询
>
>- CDN缓存
>
>  **什么是CDN？**
>
>  全称 Content Delivery Network,即内容分发网络。 各地部署多套静态存储服务，本质上是空间换时间。自动选择最近的节点内容，不存在再请求原始服务器。适合存储更新很少的静态内容，文件更新慢。
>
>  类似于火车站代售点，乘客不用再去售票大厅去排队买票 减轻了售票大厅的压力（起到分流作用,减轻服务器负载压力）。
>
>  用户在浏览网站的时候，`CDN`会选择一个离用户最近的CDN边缘节点来响应用户的请求，这样海南移动用户的请求就不会千里迢迢跑到北京电信机房的服务器（假设源站部署在北京电信机房）上了。
>
>  **CDN缓存**
>
>  当浏览器本地缓存失效,浏览器会向CDN边缘节点发起请求。类似浏览器缓存,CDN边缘节点也存在着一套缓存机制。CDN边缘节点缓存策略因服务商不同而不同，但一般都会遵循`http`标准协议，通过http响应头中的 `Cache-control: max-age`的字段来设置CDN边缘节点数据缓存时间。
>
>  CDN系统能够实时的根据网络流量和各节点的连接、负载状况以及到用户的距离和响应时间等综合信息将用户的请求重新导向离用户最近的服务节点上。
>
>  **CDN边缘节点数据缓存机制**
>
>  - 当浏览器向CDN节点请求数据时，CDN节点会判断缓存数据是否过期，
>  - 未过期：直接将缓存数据返回给客户端；
>  - 过期：CDN节点向服务器发出回源请求，拉取最新数据同时更新本地缓存，并将最新数据返回给客户端。
>
>  CDN服务商一般会提供基于文件后缀、目录多个维度来指定CDN缓存时间，为用户提供更精细化的缓存管理。
>
>  **CDN 优势**
>
>  1. CDN节点解决了跨运营商和跨地域访问的问题，访问延时大大降低。
>  2. 大部分请求在CDN边缘节点完成，CDN起到了分流作用，减轻了源服务器的负载。
>
>  传统访问：用户在浏览器输入域名发送请求-解析域名获取服务器IP地址-根据IP地址找到对应的服务器-服务器响应并返回数据。
>
>  使用CDN访问：用户发送请求-智能DNS的解析(根据IP判断地理位置、接入网类型、选择路由最短和负载最轻的服务器)-取得缓存服务器IP-把内容返回给用户(如果缓存中有)-向源站发起请求-将结果返回给用户-将结果存入缓存服务器。
>
>  **适用场景**
>
>  - 站点或者应用中大量静态资源的加速分发，例如：CSS,JS,图片和HTML
>  - 大文件下载
>  - 直播网站等
>
>  **CDN的实现**
>
>  - BAT、阿里云、腾讯云等都有提供CDN服务
>  - 可用LVS做4层负载均衡
>  - 可用Nginx，Varnish，Squid，Apache TrafficServer做7层负载均衡和cache
>  - 适用squid反向代理，或者Nginx等的反向代理
>
>- 浏览器缓存（HTTP缓存）
>
>  **访问缓存优先级**
>
>  1. 先在内存中查找,如果有,直接加载。
>  2. 如果内存中不存在,则在硬盘中查找,如果有直接加载。
>  3. 如果硬盘中也没有,那么就进行网络请求。
>  4. 请求获取的资源缓存到硬盘和内存。
>
>  **浏览器缓存的分类**
>
>  - 强缓存   (浏览器会先判断是否命中**强缓存**)
>  - 协商缓存
>
>  **浏览器缓存的优点**
>
>  1. 减少了冗余的数据传输 节省了网费
>  2. 减少了服务器的负担，大大提升了网站的性能
>  3. 加快了客户端加载网页的速度
>
>  **浏览器在第一次请求发生后，再次请求时：**
>
>  1. 验证是否命中强缓存，如果命中，就直接使用缓存了。
>  2. 如果没有命中强缓存，就发请求到服务器检查是否命中协商缓存。
>  3. 如果命中协商缓存，服务器会返回 304 告诉浏览器使用本地缓存。
>  4. 否则，返回最新的资源
>
>- 强缓存
>
>  强缓存是利用http的返回头中的`Expires`或者`Cache-Control`两个字段来控制的，用来表示资源的缓存时间
>
>  **Expires：**该字段是http1.0时的规范，它的值为一个绝对时间的GMT格式的时间字符串，比如 `Expires:Mon,18 Oct 2066 23:59:59 GMT`。这个时间代表着这个资源的失效时间，在此时间之前即**命中缓存。**
>
>  **缺点**：由于失效时间是一个绝对时间，所以当服务器与客户端时间偏差较大时，就会导致缓存混乱
>
>  **Cache-Control：**在header中设置用来控制浏览器缓存行为。
>
>  Cache-Control是http1.1时出现的header信息，主要是利用该字段的max-age值来进行判断，它是一个相对时间，例如 `Cache-Control:max-age=3600`，代表着资源的有效期是3600秒。cache-control除了该字段外，还有下面几个比较常用的设置值：
>
>  - no-cache：不使用本地缓存。需要使用缓存协商，先与服务器确认返回的响应是否被更改，如果之前的响应中存在ETag，那么请求的时候会与服务端验证，如果资源未被更改，则可以避免重新下载。
>  - no-store：直接禁止游览器缓存数据，每次用户请求该资源，都会向服务器发送一个请求，每次都会下载完整的资源。
>  - public：可以被所有的用户缓存，包括终端用户和CDN等中间代理服务器。http通信的过程中，包括请求的发起方（浏览器）、代理缓存服务器都可以进行缓存。
>  - private：只允许请求的发起方（浏览器）进行缓存，不允许CDN等中继缓存服务器对其缓存。
>
>  Cache-Control与Expires可以在服务端配置同时启用，同时启用优先级 Cache-Control > Expires
>
>- 协商缓存和304
>
>  304代表什么？什么情况下返回304？
>
>  当强缓存没有命中的时候，浏览器会发送一个请求到服务器，服务器根据 `header`中的部分信息来判断是否命中缓存。如果命中，则返回`304`，告诉浏览器资源未更新，可使用本地的缓存。
>
>  ```javascript
>  header： Last-Modify/If-Modify-Since `和 `header：``ETag/If-None-Match
>  ```
>
>  **Last-Modify/If-Modify-Since：**
>
>  - 浏览器第一次请求一个资源的时候，服务器返回的 header 中会加上 Last-Modify，Last-modify 是一个时间标识该资源的最后修改时间。 
>  - 当浏览器再次请求该资源时，request 的请求头中会包含 If-Modify-Since，该值为缓存之前返回的 Last-Modify。服务器收到 If-Modify-Since 后，根据资源的最后修改时间判断是否命中缓存。 如果命中缓存，则返回 304，并且不会返回资源内容，并且不会返回 Last-Modify。
>
>  **缺点**: 
>
>  1. **短时间内资源发生了改变**,`Last-Modified`并不会发生变化。 
>
>  2. **周期性变化**,如果这个资源在一个周期内修改回原来的样子了，我们认为是可以使用缓存的，但是 `Last-Modified`可不这样认为,因此便有了 `ETag`。
>
>  `ETag/If-None-Match`与 `Last-Modify/If-Modify-Since`不同的是：
>
>  `Etag/If-None-Match`返回的是一个校验码。`ETag`可以保证每一个资源是唯一的，资源变化都会导致 `ETag`变化。服务器根据浏览器上送的 `If-None-Match`值来判断是否命中缓存。 与 `Last-Modified`不一样的是，当服务器返回 `304 Not Modified`的响应时，由于 `ETag`重新生成过，`response header`中还会把这个 `ETag`返回，即使这个 `ETag`跟之前的没有变化。Last-Modified 与 ETag 是可以一起使用的，服务器会优先验证 ETag，一致的情况下，才会继续比对 Last-Modified，最后才决定是否返回 304。
>
>  **动态网页如何设置304**：
>
>  ```javascript
>  var request = context.Request;
>  var response = context.Response;
>  if (request.Headers["If-Modified-Since"].NotNullOrEmpty() || request.Headers["If-None-Match"].NotNullOrEmpty()) {
>    response.StatusCode = 304;
>    return;
>  }
>  //非304情况下的操作 略
>  //设置缓存选项
>  response.Clear();
>  response.ClearContent();
>  response.Headers["Last-Modified"] = DateTime.Now.ToString("yyyy-MM-dd HH:mm:ss");
>  response.Headers["ETag"] = id;//这里假设的是根据不同的id
>  response.CacheControl = "private";
>  response.ExpiresAbsolute = DateTime.Now.AddMonths(6);
>  ```
>
>- HTML5的文件离线存储怎么使用，工作原理是什么?
>
>  在线情况下，浏览器发现HTML头部有manifest属性，它会请求manifest文件，如果是第一次访问，那么浏览器就会根据manifest文件的内容下载相应的资源，并进行离线存储。如果已经访问过并且资源已经离线存储了，那么浏览器就会使用离线的资源加载页面。然后浏览器会对比新的manifest文件与旧的manifest文件，如果文件没有发生改变，就不会做任何操作，如果文件改变了，那么就会重新下载文件中的资源，并且进行离线存储
>
>  在页面头部加上：`<html manifest='cache.manifest'>`

##### 浏览器本地存储/前端持久化

>- 前端持久化的方式和区别
>
>  **一、前端持久化的方式最常用的两种：**
>
>  1.使用前端cookie技术来保存本地化数据，如jquery.cookie.js；
>
>  2.使用html5提供的Web Storage技术来提供解决方案；
>
>  **二、cookie存储永久数据的问题**
>
>  用cookie存储永久数据存在以下几个问题：
>
>  1.大小：cookie的大小被限制在4KB，个数不超过150个
>
>  2.带宽：cookie是随HTTP事务一起被发送的，因此会浪费一部分发送cookie时使用的带宽。
>
>  3.复杂性：要正确的操纵cookie是很困难的
>
>  HTTP设置cookie时，提供了2个属性，可以增强cookie的安全性，分别是secure属性和httpOnly属性。
>
>  - secure 属性可防止信息在传递的过程中被监听捕获后导致信息泄露。如果设置为true，可以限制只有通过https访问时，才会将浏览器保存的cookie传递到服务端；如果通过http访问，不会传递cookie。
>  - httpOnly 属性可以防止程序获取cookie，如果设置为true，通过js等将无法读取到cookie，能有效的防止XSS攻击。
>
>  **三、Web Storage介绍**
>
>  针对这些问题，在HTML5中，重新提供了一种在客户端本地保存数据的功能，它就是Web Storage。
>
>  具体来说，Web Storage又分为两种：
>
>  1.sessionStorage：将数据保存在session对象中。所谓session，是指用户在浏览某个网站时，从进入网站到浏览器关闭所经过的这段时间，也就是用户浏览这个网站所花费的时间。session对象可以用来保存在这段时间内所要求保存的任何数据。
>
>  2.localStorage：将数据保存在客户端本地的硬件设备(通常指硬盘，也可以是其他硬件设备)中，即使浏览器被关闭了，该数据仍然存在，下次打开浏览器访问网站时仍然可以继续使用。
>
>  这两者的区别在于，sessionStorage为临时保存，而localStorage为永久保存
>
>  - **服务端的session登录态会话，一般是如何通过cookie来保持的？过期时间？有效域？**
>
>    - **Domain**: cookie有效域
>
>      - 如果一个cookie显示设置domain，则该domain下所有子域网页都可以访问。例如当前有一个网页**http://a.shifei.com/a.html**,`document.cookie="name=shifei;domain=.shifei.com";`
>      - 没有显示设置domain时，默认为当前网页的host。
>
>    - **Path** 该属性设置cookie允许哪些目录被访问
>
>      - 如当前网页为[http://a.shifei.com/news/a.html](https://link.jianshu.com/?t=http://a.shifei.com/news/a.html)`documen.cookie="name=shifei;path=/";`
>
>    - **Max-age** : cookie有效期
>
>      max-age属性用来代替旧的属性expires，如果浏览器支持max-age，那么优先使用max-age设置的过期时间，否则，仍然使用expires作为过期时间
>
>    - **expires**  
>
>      cookie的过期时间，用GMT或者UTC时间格式来表示。
>
>      cookie从创建之初到expires设置的时间内是存活期，如果当前时间大于expires的时间，则该cookie被删除。
>
>    - **httpOnly**
>
>      该属性设置cookie是否可以通过javascript代码来进行访问。
>
>      js是无法设置该属性的，只能是服务端进行设置。
>
>      该属性目的是为了防止cookie在传输过程中被篡改，提高网页安全性，能有效的防止XSS攻击。
>
>    - **secure**
>
>      设置cookie只在确保安全的请求中传送。
>
>      当请求是*HTTPS*或者其他的安全协议时，带有*secure*属性的cookie才 能被发送到服务器。
>
>      默认情况，cookie不会带有secure属性，所以，如果没有显示设置secure属性的cookie，在*HTTPS*和*HTTP*协议下，都会被发送到服务端。
>
>      有一点需要说明，带有secure的cookie，只是在安全协议下才能被发送到服务端，但是，我们仍然可以通过浏览器查看到该cookie的
>
>- localStorage/ sesionStorage区别
>
>  - localStorage API
>
>    - `localStorage.setItem('a', 100);  localStorage.getItem('a')`
>
>  - 如何设计一个localStorage，设置有效期？
>
>    - 重写set 方法
>
>      - 首先有三个参数 key、value、expired ，分别对应 键、值、过期时间，
>      - 过期时间的单位可以自由发挥，小时、分钟、天都可以，
>      - **注意**点：存储的值可能是数组/对象，不能直接存储，需要转换 `JSON.stringify`，
>      - 这个时间如何设置呢？在这个值存入的时候在键(key)的基础上扩展一个字段，如：key+‘*expires*’，而它的值为当前 时间戳 + expired过期时间
>
>      ```javascript
>      //	key 键 	value 值	expired 过期时间，以分钟为单位，非必须
>      set(key, value, expired) {
>      	let source = this.source;
>      	source[key] = JSON.stringify(value); //存储的值可能是对象或者数组，不能直接存储
>      	if (expired){
>      		source[`${key}__expires__`] = Date.now() + 1000*60*expired // expired分钟为单位
>      	};
>      	return value;
>      }
>      ```
>
>    - 重写get方法：
>
>      - 获取数据时，先判断之前存储的时间有效期，与当前的时间进行对比；
>      - 但存储时`expired`为非必须参数，所以默认为当前时间+1，即长期有效；
>      - 如果存储时有设置过期时间，且在获取的时候发现已经小于当前时间戳，则执行删除操作，并返回空值；
>      - **注意**点：存储的值可能是数组/对象，取出后不能直接返回，需要转换 `JSON.parse`
>
>      ```javascript
>      get(key){
>      	const source = this.source;
>        const expired = source[`${key}__expires__`] || Date.now() +1
>        const now = date.now();
>        if(now >= expired){
>          this.remove(key)
>          return
>        }
>        return souce[key] ? JSON.parse(source[key]): source[key]
>      }
>      //重写remove
>      remove(key) {
>      	const source = this.source, value = source[key];
>      	delete source[key];
>      	delete source[`${key}__expires__`];
>      	return value;
>      }
>      ```
>
>  - localStorage优点
>
>    - 在浏览器关闭后也存在，适用于页面会有一些持久化的操作，这样最大限度的减少数据库的访问量。
>    - 存储一些需要刷新保存并且需要在页面关闭后仍然留下的信息。
>    - 如，可用于保存购物车中的内容；在之前项目中，用于保存上一次的用户浏览标签，并跳转到相应的路径下
>
>  - sessionStorage优点
>
>    - 在浏览器关闭后就会清理缓存，在多个窗口下也是不同的，适合做一些当首次进入页面的时候必须要和后台进行交互的场景。
>    - 存储一些当前页面刷新需要存储，且不需要在tab关闭时候留下的信息。
>    - 如，可用来检测用户是否是刷新进入的页面，如音乐播放器恢复播放进度条的功能。 非常适合单页应用程序，可以方便在各业务模块进行传值
>
>- Cookie session token
>
>  - **Cookie与Session的原因与概念** 
>
>    Session比Cookie安全，Session是存储在服务器端的，Cookie是存储在客户端的。
>
>    - 原因：由于HTTP协议是无状态协议；
>
>      HTTP 协议是一种`无状态协议`，即每次服务端接收到客户端的请求时，都是一个全新的请求，它不能以状态来区分和管理请求和响应。也就是说，服务器单从网络连接上无从知道客户身份, 也并不知道客户端的历史请求记录。Session 和 Cookie 的主要目的就是为了弥补 HTTP 的无状态特性。
>
>    - Cookie：服务器端想识别特定的用户时，Cookie可以用来识别用户，每次HTTP请求时，客户端都会发送相应的Cookie信息到服务器端；
>
>    - Session：当服务器需要记录用户的状态时，需要Session机制来识别具体的用户。例如购物车，当用户点击下单时，服务器要为特定用户创建Session，用于标识这个用户并跟踪，Session是保存在服务器端的，有唯一标识
>
>  - **Cookie与Session的区别** 
>
>    1. 存放位置不同：cookie数据存放在客户的浏览器（客户端）上，session数据放在服务器上，但是服务端的session的实现对客户端的cookie有依赖关系的；
>    2. 安全性不同：cookie不是很安全，别人可以分析存放在本地的COOKIE并进行COOKIE欺骗，考虑到安全应当使用session；
>    3. 占用服务器性能：session会在一定时间内保存在服务器上。当访问增多，会比较占用你服务器的性能。考虑到减轻服务器性能方面，应当使用COOKIE；
>    4. 单个cookie在客户端的限制是3K，就是说一个站点在客户端存放的COOKIE不能超过3K
>
>  - **cookie工作原理：**
>
>    客户端第一次访问服务器, 服务器端保存客户的信息并且给客户端一个Cookie, 客户端携带Cookie去访问服务端, 服务端通过携带的Cookie找出该用户信息. 服务端就能够知道是谁访问了。
>
>    保存方式：cookie由服务器生成，保存在客户端浏览器。
>
>    缺点：容易被劫持，不安全
>
>  - session 从字面上讲，就是会话。服务器要知道当前发请求给自己的是谁。这个就类似于你和一个人交谈，你怎么知道当前和你交谈的是张三而不是李四呢？对方肯定有某种特征（长相等）表明他就是张三。
>
>    保存方式：session保存在服务端
>
>    缺点：当用户量太大时，占用服务器资源。
>
>    优点：较安全
>
>  - **token工作原理**
>
>    用户通过用户名和密码向服务端发送请求；服务端通过验证，生成一个token发送给客户端；客户端保存token，发送请求时带上token；服务器通过验证，返回数据。
>
>    token的优势：
>
>    1. 无状态、可扩展，可以在多个服务间共享
>    2. 支持移动设备
>    3. 跨程序调用
>    4. 安全， Token可以避免 CSRF 攻击；
>    5. Token完全由应用管理，所以它可以避开同源策略；
>
>  - **cookie放哪里，Cookie能做的事情和存在的价值**
>
>    - Chrome的Cookie数据位于：%LOCALAPPDATA%\Google\Chrome\User Data\Default\ 目录中，名为Cookies的文件。
>    - cookie存放在浏览器的header中
>    - cookie的最初设计本身用于浏览器和server通讯，被“借用”来做本地存储
>
>  - **Cookie和Token都存放在Header里面，为什么不会劫持token**
>
>    - CSRF攻击的原因是浏览器会自动带上cookie，而不会带上token；
>
>    - - 浏览器发送请求的时候不会自动带上token，而cookie在浏览器发送请求的时候会被自动带上。
>
>    - CSRF就是利用的这一特性，所以token可以防范csrf，而cookie不能。 （token不是为了防止XSS的，而是为了防止CSRF的）
>    - 以CSRF攻击为例：
>
>    - - cookie：用户点击链接，cookie未失效，导致发起请求后后端以为是用户正常操作，于是进行扣款操作；
>      - token：用户点击链接，由于浏览器不会自动带上token，所以即使发了请求，后端的token验证不会通过，所以不会进行扣款操作；
>
>    - JWT(JSON Web Token) 本身只关心请求的安全性，并不关心toekn本身的安全

##### 即时通信技术

>- **短轮询  --- xhr实现**
>
>  短轮询的基本思路就是浏览器每隔一段时间向浏览器发送http请求，服务器端在收到请求后，不论是否有数据更新，都直接进行响应。
>
>  优点: 比较简单，易于理解，实现起来也没有什么技术难点。
>
>  缺点: 需不断的建立http连接，严重浪费了服务器端和客户端的资源。如 每一个用户的客户端都会疯狂的向服务器端发送http请求，而且不会间断。人数越多，服务器端压力越大，这是很不合理的。
>
>  因此短轮询不适用于那些同时在线用户数量比较大，并且很注重性能的Web应用
>
>  ```javascript
>  var xhr = new XMLHttpRequest();
>  setInterval(function(){
>    xhr.open('GET','/user');
>    xhr.onreadystatechange = function(){ /* ... */ };
>    xhr.send();
>  },1000)
>  ```
>
>- **长轮询（comet）  --- ajax实现**
>
>  当服务器收到客户端发来的请求后,服务器端不会直接进行响应，而是先将这个请求挂起，然后判断服务器端数据是否有更新。如果有更新，则进行响应，如果一直没有数据，则到达一定的时间限制(服务器端设置)才返回。 。客户端JavaScript响应处理函数会在处理完服务器返回的信息后，再次发出请求，重新建立连接。
>
>  长轮询和短轮询比起来，明显减少了很多不必要的http请求次数，相比之下节约了资源。长轮询的缺点在于，连接挂起也会导致资源的浪费
>
>  ```javascript
>  function ajax(){
>    var xhr = new XMLHttpRequest();
>    xhr.open('GET','/user');
>    xhr.onreadystatechange = function(){
>      ajax();
>    };
>    xhr.send();
>  }
>  ```
>
>- **长链接(SSE)**
>
>  SSE是HTML5新增的功能，全称为Server-Sent Events。它可以允许服务推送数据到客户端。SSE在本质上就与之前的长轮询、短轮询不同，虽然都是基于http协议的，但是轮询需要客户端先发送请求。而SSE最大的特点就是不需要客户端发送请求，可以实现只要服务器端数据有更新，就可以马上发送到客户端。
>
>  SSE的优势很明显，它不需要建立或保持大量的客户端发往服务器端的请求，节约了很多资源，提升应用性能。SSE的实现非常简单，并且不需要依赖其他插件
>
>- **websocket**
>
>  WebSocket是Html5定义的一个新协议，与传统的http协议不同，该协议可以实现服务器与客户端之间全双工通信。简单来说，首先需要在客户端和服务器端建立起一个连接，这部分需要http。连接一旦建立，客户端和服务器端就处于平等的地位，可以相互发送数据，不存在请求和响应的区别。
>
>  WebSocket的优点是实现了双向通信，缺点是服务器端的逻辑非常复杂。现在针对不同的后台语言有不同的插件可以使用
>
>  - websocket应用层协议
>
>    Websocket 是一个 **持久化的协议**，服务端可以 **主动 push。**webSocket 和 http一样，同属于应用层协议。用途是实现了客户端与服务端之间的全双工通信，当服务端数据变化时，可以第一时间通知到客户端。
>
>    - http只能由客户端发起，而webSocket是双向的。
>    - webSocket传输的数据包相对于http而言很小，很适合移动端使用
>    - 没有同源限制，可以跨域共享资源
>
>    **WebSocket的原理**：
>
>    - 网页打开后 浏览器向客户端建立一个**TCP连接(WebSocket协议)**，双方的此连接便不再断开, 这样双方都可以主动向对方发消息了(主要是这样服务端就能主动向客户端发消息啦~)
>    - 此处客户端的确也是先发送一个HTTP请求给服务端，该请求要求服务器 将该HTTP连接升级为WS连接，在服务器端响应ok后 它们之间便不再走HTTP报文了; 之后该连接上走websocket协议，并且不会断开，信息在两端交换。连接一旦建立，客户端和服务器端就处于平等的地位，可以相互发送数据，不存在请求和响应的区别。
>
>    websocket协议是以消息为单位--有点像UDP，但websocket是可靠 并且有序的。
>
>    - UDP是任意两个端口之间可以随意的发送，websocket则是只能在建立连接的两端交换消息(交换的是消息，而不是字节流)

##### 跨域

>- 浏览器的同源策略是什么？在相同域的情况下，http的页面能跨域请求https的吗？反之呢？有哪些跨域方案？
>
>  **设置同源策略的目的：**
>
>  设置同源限制主要是为了安全，如果没有同源限制存在浏览器中的cookie等其他数据可以任意读取，不同域下DOM任意操作，ajax任意请求的话如果浏览了恶意网站那么就会泄漏这些隐私数据。
>
>  **浏览器的同源策略是什么？**
>
>  - 同源策略是浏览器最核心也最基本的安全功能，如果缺少了同源策略，浏览器很容易受到XSS、CSFR等攻击。
>  - **ajax请求**时，**浏览器**要求当前网页和server必须同源 (安全)；
>    这里仅限浏览器，而搜索引擎、爬虫是服务端去请求访问网页的，这就可以拿到数据。(所以可在server端发起攻击，攻击不同源网站的接口)
>  - 同源：协议(http/https)、域名、端口，三者必须一致；(即便两个不同的域名指向同一个ip地址，也非同源)
>  - 前端：http://a.com:8080/; server: https://b.com/api/xxx（默认端口是80）  => 这里的前端与server地址，协议 域名 端口 都不一致。
>
>  **在相同域的情况下，http的页面能跨域请求https的吗？**
>
>  不能，同源策略要求的同源必须是 协议、域名、端口，三者都一致
>
>  **在相同域的情况下，https的页面能跨域请求http的吗？**
>
>  浏览器默认是不允许在HTTPS里面引用HTTP资源的，如果加载了http资源，浏览器将认为这是不安全的资源，将会默认阻止，这就会给你带来资源不全的问题了，比如：在一个HTTPS页面里动态的引入HTTP资源，导致 图片显示不了，样式加载不了，JS加载不了**。**
>
>  **跨域方案**
>
>  - 通过jsonp跨域  (一定要理解JSONP的原理, 其是怎样实现的)
>
>    利用`<script>`标签不受跨域限制的特点，缺点是只能支持 get 请求
>
>    **JSONP基本的实现原理**：
>
>    - `<script>`可绕过跨域限制；(于是script返回的地址就是跨域的地址)
>    - 服务器可以任意**动态拼接**数据返回；
>    - 所以，`<script>`就可以获得跨域的数据，只要服务端愿意返回。(跨域必须要经过server端的允许和配合)
>
>    **JSONP缺点**：只能实现get一种请求。
>
>    **JSONP的使用**：
>
>    - `<script src=跨域的js地址></script>`
>    - 通常为了减轻web服务器的负载，我们把js、css，img等静态资源分离到另一台独立域名的服务器上，在html页面中再通过相应的标签从不同域名下加载静态资源，而被浏览器允许，基于此原理，我们可以通过动态创建script，再请求一个带参网址实现跨域通信。
>    - 如在自己的网站上用CDN的地址(外域)；Content Delivery Network，即内容分发网络。
>
>    **实现方式：**
>
>    - 动态拼接username的内容返回，callback函数名字也不是固定的；<script>标签也可动态的插入
>
>    如：`<script src="https://localhost:8002/jsonp.js?username=xxx&callback=abc"></script>`;
>
>    - 同理于 `<script src="https://imooc.com/getData.js">`;
>      访问js文件，并不一定返回给你一个静态的js文件，而是服务端可通过拼接任何的数据 返回给你，只需服务端返回的数据是js的格式。
>
>    ```javascript
>    function jsonp(url, jsonpCallback, success) {
>      const script = document.createElement('script')
>      script.src = url
>      script.async = true
>      script.type = 'text/javascript'
>      window[jsonpCallback] = function(data) {
>        success && success(data)
>      }
>      document.body.appendChild(script)
>    }
>    ```
>
>  - document.domain + iframe跨域
>
>  - location.hash + iframe
>
>  - window.name + iframe跨域
>
>  - postMessage跨域
>
>  - 跨域资源共享 CORS (纯服务端去做的 --nodejs开发)
>
>    CORS是另一种跨域的形式, 是纯服务器端的操作；若服务器端用CORS允许了某个域的跨域访问，那就不用在前端用JSONP跨域了 直接用ajax访问 无需考虑跨域问题 。
>
>    - 普通跨域请求，服务端设置 `Access-Control-Allow-Origin：*`即可，前端无须设置。
>    - 若要带cookie请求：前后端都需要设置。
>
>    - - 前端还需设置 `xhr.withCredentials = true;` (前端设置是否带cookie)
>      - nodejs设置 `'Access-Control-Allow-Credentials': 'true',`    // 后端允许发送Cookie！
>
>    `'Access-Control-Allow-Origin': 'http://www.domain1.com',`  // 允许访问的域
>
>    `'Set-Cookie': 'l=a123456;Path=/;Domain=www.domain2.com;HttpOnly'`  // HttpOnly-让js无法读取cookie
>
>    **需注意的是**：
>
>    - 由于同源策略的限制，CORS 所读取的cookie为跨域请求接口所在域的cookie，而非当前页。
>    - 如果想实现当前页cookie的写入，可参考下文：nginx反向代理中设置 `proxy_cookie_domain`和 NodeJs中间件代理中 `cookieDomainRewrite`参数的设置。
>
>    目前，所有浏览器都支持该功能(IE8+：IE8/9需要使用XDomainRequest对象来支持CORS）)，CORS也已经成为主流的跨域解决方案
>
>  - nginx代理跨域
>
>  - nodejs中间件代理跨域
>
>  - WebSocket协议跨域
>
>- 在https协议打开的H5页面内，有某些JS或CSS资源会被block，这是什么原因？图片资源会吗
>
>  **1.加载图片, css, js 可无视同源策略**：（不是ajax请求）
>
>  - `<img src=跨域的图片地址/>`
>
>  - - 图片可能出现防盗链的情况；**防盗链**是图片服务器做的，而不是浏览器做的。（**同源策略**是浏览器的限制）
>
>  - `<link href=跨域的css地址 />`
>  - `<script src=跨域的js地址></script>`  如在自己的网站上用CDN的地址；
>
>  **2.在https的网站中引用http路径的js或css会导致不起作用**，其形如：
>
>  <script src="http://code.jquery.com/jquery-1.11.0.min.js"></script>
>
>  **解决办法：**
>
>  将http:去掉，改为  `<script src="//code.jquery.com/jquery-1.11.0.min.js"></script>`
>
>  浏览器默认是不允许在HTTPS里面引用HTTP资源的，如果在一个HTTPS页面里动态的引入HTTP资源，比如引入一个js文件，会被直接block掉的。
>
>  浏览器为了安全，https下跨协议调用http的是不行的，控制台里会有警告。
>
>  所以只能将css、js资源改为https协议。也有文章说https页面中可引用http的图片，发现虽然不会报红错，但是会有黄色的提醒。最好的方法是使用https的资源。
>
>  **3.在https的网站中引用图片资源会受影响吗？---  一样会受影响**
>
>  https地址中，如果加载了http资源，浏览器将认为这是不安全的资源，将会默认阻止，这就会给你带来资源不全的问题了，比如：图片显示不了，样式加载不了，JS加载不了**。**因为样式类，基本上都是写在本地的，所以一般还可以，但是一些公共的js文件，往往就是存在于cdn或者其他服务器上，这时候，如果访问不了，可能就导致了业务就完全操作不了。比如：jquery效法加载失败，可能所有的操作、请求都将无效了

##### 抓包

##### 前端安全

###### 跨站脚本攻击（XSS）

> **跨站脚本攻击**，Cross Site Script（简称 CSS或）。指黑客通过“**HTML注入**”篡改了网页，插入了恶意的脚本（主要是**JavaScript脚本**），从而在用户浏览网页时，控制用户浏览器的一种攻击。
>
> xss的危害：挂马，盗取用户cookie，钓鱼攻击，劫持用户web行为，爆发web2.0若虫，刷广告等
>
> 常见的跨站脚本攻击也可以分为：反射型xss、存储型xss、DOM Based XSS
>
> - 反射型XSS
>
> 简单的把用户输入的数据“反射”给浏览器，即黑客往往需要诱惑用户点击一个恶意的链接攻击才能成功，用户通过点击这个链接，攻击者可以成功获取到用户的隐私数据。如盗取用户的cookie信息，破坏页面结构、重定向到其他网站
>
> 既然反射型XSS可以是html注入，那么它注入的关键自然也从前端的html页面开始下手：
>
> 1. 用户能够与浏览器页面产生交互动作（如输入搜索的关键词、点击按钮、点击链接）
> 2. 用户输入的数据会被攻击方拼接成合适的html去执行恶意的js脚本
>
> - 存储型XSS 
>
> 也称为持久性XSS,与反射型的不同在于会将用户输入的数据存储在攻击方的服务器上，具有很强的稳定性。
>
> 例如：访问某黑客写下的一篇含有恶意js代码的博客，黑客把恶意脚本保存到服务端。
>
> - DOM Based XSS
>
> 从效果上说也是反射型，单独划分是因为其形成是通过修改页面的dom节点形成的XSS
>
> 例如：通过修改DOM节点上的绑定方法，用户无意间点击、输入等行为执行这些方法获取用户的信息
>
> - XSS的攻击方式
> - cookie劫持
>
> 通过伪装一些`图片和按钮`等，诱使用户对其操作，使网页执行了攻击者的恶意脚本，使攻击者能够获取当前用户的Cookie信息
>
> - 构造get和post请求
>
> 若某攻击者想删除某网站的一篇文章，首先获得当前文章的id，然后通过使用脚本`插入图片`发送一个`GET请求`，或`构造表单`，`XMLHTTPRequest`发送`POST请求`以达到删除该文章的目的
>
> - XSS钓鱼
>
> `钓鱼`这个词一般认识是起源于`社会工程学`，黑客使用这个这门学科的理念思想，在未授权不知情的情况下诱骗用户，并得到对方对方的姓名、年龄、邮箱账号、甚至是银行卡密码等私人信息。
>
> 比如："某用户在某网站（已被攻击）上操作黑客伪造的一个登录框，当用户在登录框中输入了用户名（这里可能是身份证号等）和密码之后，将其信息上传至黑客的服务器上（该用户的信息就已经从该网站泄漏）"
>
> - 获取用户真实的IP地址
>
> 通过第三方软件获取，比如客户端安装了Java环境（JRE），则可通过调用`Java Applet`的接口获取客户端本地的IP地址
>
> - XSS攻击的防御措施
> - HttpOnly
>
> 原理：浏览器禁止页面的Javascript访问带有HttpOnly属性的cookie。（实质解决的是：XSS后的cookie劫持攻击）如今已成为一种“标准”的做法
>
> 解决方案：
> JavaEE给Cookie添加HttpOnly的方式为：
> `response.setHeader("Set-Cookie","cookiename=value; Path=/;Domain=domainvalue;Max-Age=seconds;HTTPOnly");`
>
> - 输入检查（XSS Filter）
>
> 原理：让一些基于特殊字符的攻击失效。（常见的Web漏洞如XSS、SQLInjection等，都要求攻击者构造一些特殊字符）
>
> - 输入检查的逻辑，必须在服务端实现，因为客户端的检查也是很容易被攻击者绕过，现有的普遍做法是两端都做同样的检查，客户端的检查可以阻挡大部分误操作的正常用户，从而节约服务器的资源。
>
> 解决方案：
> 检查是否包含"JavaScript"，"<script></script>"等敏感字符。以及对字符串中的<>:"&/'等特殊字符做处理
>
> - 输出检查
>
> 原理：一般来说除了富文本输出之外，在变量输出到HTML页面时，使用编码或转义的方式来防御XSS攻击
>
> 解决方案：
>
> - 针对HTML代码的编码方式：HtmlEncode
> - PHP：htmlentities()和htmlspecialchars()两个函数
> - Javascript：JavascriptEncode（需要使用""对特殊字符进行转义，同时要求输出的变量必须在引号内部）
> - 在URL的path（路径）或者search（参数）中输出，使用URLEncode

###### 跨站点请求伪造（CSRF）

> Cross Sites Request Forgery,跨站点请求伪造，指利用用户身份操作用户账户的一种攻击方式，即攻击者诱使用户访问一个页面，就以该用户身份在第三方有害站点中执行了一次操作，泄露了用户的身份信息，接着攻击者就可以使用这个伪造的，但真实存在的身份信息，到某网站冒充用户执行恶意操作
>
> 但是，攻击者只有预测到URL的所有参数与参数值，才能成功地伪造一个请求（当然了，他可以在安全站点里以自己的身份实际去操作一下，还是能拿到参数的）；反之，攻击者无法攻击成功
>
> ![image-20201113144815236](/Users/banggan/Library/Application Support/typora-user-images/image-20201113144815236.png)
>
> 参考上图，我们可以总结，完成一次CSRF攻击，必须满足两个条件：1.用户登录受信任网站A，并且在本地生成Cookie；2. 在不登出网站A的情况下，访问有害网站B
>
> - CSRF的原理
>
> CSRF攻击是攻击者利用**`用户身份`**操作用户账户的一种攻击方式
>
> 如电影速度与激情5中吉赛尔使用内裤获取巴西大佬指纹，最后成功使用伪造指纹的手法打开保险柜，CSRF只不过是网络上这个手法的实现。
>
> - CSRF的攻击方式
> - 浏览器的cookie策略
>
> 浏览器所持有的策略一般分为两种：
> Session Cookie，临时Cookie。保存在浏览器进程的内存中，浏览器关闭了即失效。
> Third-party Cookie，本地Cookie。服务器在Set-Cookie时指定了Expire Time。过期了本地Cookie失效，则网站会要求用户重新登录。
>
> 在浏览网站的过程中，即使浏览器打开了Tab页，Session Cookie都是有效的，因此发起CSRF攻击是可行的
>
> - P3P头的副作用
>
> "P3P Header"是 "W3C" 制定的一项关于隐私标准，全称 "The Platform for Privacy Preference"（隐私偏好平台）
>
> 如果网站返回给浏览器的 HTTP 头包含有 P3P 头，则在某种程度上来说，将允许 浏览器发送第三方 Cookie。在 IE 下即使是"<iframe>"、`<script>`等标签页将不再拦截第三方 Cookie 的发送。主要应用在类似广告等需要跨域访问的页面。
>
> - GET POST请求
>
> 大多数 CSRF 攻击，都是通过 <img> 、 <iframe> 、 <script> 等带 src 属性的标签，这类标签只能发送一次 GET 请求，而不能发送 POST 请求，由此也有了认为 CSRF 攻击只能由 GET 请求发起的错误观点。
>
> 构造一个 POST 请求，只需要在一个不可见的iframe窗口中，构造一个form表单，然后使用JavaScript自动提交这个表单。那么整个自动提交表单的过程，对于用户来说就是不可见的。
>
> - CSRF攻击的防御方式
> - 验证码
>   CSRF攻击过程中，用户在不知情的情况下构造了网络请求，添加验证码后，强制用户必须与应用进行交互
>   - 优点：简洁而有效
>   - 缺点：网站不能给所有的操作都加上验证码
> - Referer check
>   - 利用HTTP头中的Referer判断请求来源是否合法
>   - Referer首部包含了当前请求页面的来源页面的地址，一般情况下Referer的来源页就是发起请求的那个页面，如果是在iframe中发起的请求，那么对应的页面URL就是iframe的src
>   - 优点：简单易操作（只需要在最后给所有安全敏感的请求统一添加一个拦截器来检查Referer的值就行）
>   - 缺点：服务器并非什么时候都能取到Referer
>     1.很多出于保护用户隐私的考虑，限制了Referer的发送。
>     2.比如从HTTPS跳转到HTTP，出于安全的考虑，浏览器不会发送Referer
> - 使用Anti CSRF Token
>
> 原理：把参数加密，或者使用一些随机数，从而让攻击者无法猜测到参数值，也就无法构造请求的 URL，也就无法发起 CSRF 攻击。
>
> 例子（增加token）：
>
> - 比如一个删除操作的URL是：`http://host/path/delete?uesrname=abc&item=123`
> - 保持原参数不变，新增一个参数Token，Token值是随机的，不可预测
> - http://host/path/delete?username=abc&item=123&token=[random(seed)]
> - 优点：比检查Referer方法更安全，并且不涉及用户隐私
> - 缺点：
>   1. 加密后的URL非常难读，对用户非常不友好
>   2. 加密的参数每次都在改变，导致用户无法对页面进行搜索
>   3. 普通参数也会被加密或哈希，将会给DBA工作带来很大的困扰，因为数据分析常常需要用到参数的明文    

###### 点击劫持

> 点击劫持是一种视觉上的欺骗手段。攻击者使用一个透明的、不可见的iframe，覆盖在一个网页上，然后诱使用户在网页上进行操作，此时用户将在不知情的情况下点击透明的iframe页面。通过调整iframe页面的位置，可以诱使用户恰好点击在iframe页面的一些功能性按钮上
>
> 比如，程序员小王在访问A网页时，点击空白区域，浏览器却意外打开了xx新葡京赌场的页面，于是他在A网页打开控制台，在空白区域发现了一个透明的iframe，该iframe嵌入了一个第三方网页的URL
>
> - 点击劫持的防御
> - X-Frame-Options HTTP响应头是用来给浏览器指示允许一个页面能否在`<frame>、<iframe>、<object>`中展现的标记
>
> 有三个可选的值
>
> 1. DENY：浏览器会拒绝当前页面加载任何frame页面（即使是相同域名的页面也不允许）
> 2. SAMEORIGIN：允许加载frame页面，但是frame页面的地址只能为同源域名下的页面
> 3. ALLOW-FROM：可以加载指定来源的frame页面（可以定义frame页面的地址）
>
> - 禁止iframe的嵌套
>   `if(window.top.location !== window.loaction){window.top.location === window.self.location}`

###### 其他安全

> - 跨域问题处理
>
> 当服务端设置 'Access-Control-Allow-Origin' 时使用了通配符 "*",允许来自任意域的跨域请求，这是极其危险的
>
> - postMessage 跨窗口传递信息
>   postMessage 允许每一个 window（包括当前窗口、弹出窗口、iframes等）对象往其他的窗口发送文本消息，从而实现跨窗口的消息传递。并且这个功能不受同源策略限制。
>   必要时，在接受窗口验证 Domain，甚至验证URL，以防止来自非法页面的消息。实际上是在代码上实现一次同源策略的验证过程。接受窗口对接口的信息进行安全检查。
> - Web Storage
>   Web Storage 分为 Session Storage 和 Local Storage。虽然受同源策略的约束，但当存有敏感信息时，也可能会成为攻击的目标。

##### 其他

>- 正向代理
>
>  正向代理 是一个位于客户端和原始服务器(origin server)之间的服务器，为了从原始服务器取得内容，客户端向代理发送一个请求并指定目标(原始服务器)，然后代理向原始服务器转交请求并将获得的内容返回给客户端。客户端必须要进行一些特别的设置才能使用正向代理。
>
>  **正向代理的用途：**
>
>  1. 访问原来无法访问的资源，如google
>  2. 可以做缓存，加速访问资源
>  3. 对客户端访问授权，上网进行认证
>  4. 代理可以记录用户访问记录（上网行为管理），对外隐藏用户信息
>
>- 反向代理
>
>  客户端是无感知代理的存在的，反向代理对外都是透明的，访问者者并不知道自己访问的是一个代理。因为客户端不需要任何配置就可以访问。
>
>  反向代理（Reverse Proxy）是指以代理服务器来接受internet上的连接请求，然后将请求转发给内部网络上的服务器，并将从服务器上得到的结果返回给internet上请求连接的客户端，此时代理服务器对外就表现为一个服务器。
>
>  反向代理的作用：
>
>  1. 保证内网的安全，可以使用反向代理提供WAF功能，阻止web攻击
>  2. 负载均衡，通过反向代理服务器来优化网站的负载
>
>  ![image-20201117204053423](/Users/banggan/Library/Application Support/typora-user-images/image-20201117204053423.png)
>
>  正向代理中，proxy和client同属一个LAN，对server透明；
>
>  反向代理中，proxy和server同属一个LAN，对client透明。
>
>  实际上，proxy在两种代理中做的事都是代为收发请求和响应，不过从结构上来看正好左右互换了下，所以把出现的那种代理方式叫成反向代理
>
>- 文件上传如何做断点续传
>
>  XMLHttpRequest实现HTTP协议下文件上传断点续传](https://www.zhangxinxu.com/wordpress/2013/11/xmlhttprequest-ajax-localstorage-文件断点续传)
>
>  目前从实用技术角度讲，文件上传的断点续传实现主要是借助客户端。
>
>  Ajax 2.0中最大的变化之一就是对二进制数据的支持，而且提供了一个可直接处理二进制数据的方法——`slice`方法。JS中的字符串有`slice`方法，数组也有。于是，我们就可以把二进制数据流想象成一些连续的字符串数据，并对这些二进制数据进行`slice`处理。
>
>- 大文件上传
>
>  - 利用` Blob.prototype.slice`方法，返回`原文件的某个切片`
>  - 根据预先设置好的切片最大数量将文件切分为一个个切片，然后借助http 的可并发性，同时上传多个切片，这样从原本传一个大文件，变成了同时传多个小的文件切片，可以大大减少上传时间。另外由于是并发，传输到服务端的顺序可能会发生变化，所以还需要给每个切片记录顺序。最后发送一个合并的请求通知服务端合并切片。
>
>  - 服务端接收切片并存储，收到合并请求后使用nodejs的读写流将切片合并到最终文件
>  - 原生 XMLHttpRequest 的 upload.onprogress 对切片上传进度的监听
>  - 使用 Vue 计算属性根据每个切片的进度算出整个文件的上传进度
>
>  ```javascript
>  //请求逻辑
>  request({url,method = "post",data,headers = {},requestList}) {
>        return new Promise(resolve => {
>          const xhr = new XMLHttpRequest();
>          xhr.open(method, url);
>          Object.keys(headers).forEach(key =>
>            xhr.setRequestHeader(key, headers[key])
>          );
>          xhr.send(data);
>          xhr.onload = e => {
>            resolve({
>              data: e.target.response
>            });
>          };
>        });
>  }
>  
>  //上传切片
>  //上传主要做两件事，对文件进行切片，将切片传输给服务端
>  const SIZE = 10 * 1024 * 1024; // 切片大小
>  createFileChunk(file,size = SIZE){
>    const fileChunkList = [];
>    let cur = 0;
>    while(cur<file.size){
>      fileChunkList.push({file:file.slice(cur,cur+size)})
>      cur += size
>    }
>    return fileChunkList;
>  }
>  //点击上传
>  async handleUpload() {
>        if (!this.container.file) return;
>        this.status = Status.uploading;
>        const fileChunkList = this.createFileChunk(this.container.file);
>        this.container.hash = await this.calculateHash(fileChunkList);
>        const { shouldUpload, uploadedList } = await this.verifyUpload(
>          this.container.file.name,
>          this.container.hash
>        );
>        if (!shouldUpload) {
>          this.$message.success("秒传：上传成功");
>          this.status = Status.wait;
>          return;
>        }
>        this.data = fileChunkList.map(({ file }, index) => ({
>          fileHash: this.container.hash,
>          index,
>          hash: this.container.hash + "-" + index,
>          chunk: file,
>          size: file.size,
>          percentage: uploadedList.includes(index) ? 100 : 0
>        }));
>        await this.uploadChunks(uploadedList);
>  },
>  // 上传切片，同时过滤已上传的切片
>  async uploadChucks(){
>    const requestList = this.data
>    .filter(({hash})=> uploadedList.includes(hash))
>    .map(({chuck,hash,index})=>{
>      const formData = new formData();
>      formData.append("chunk", chunk);
>      formData.append("hash", hash);
>      formData.append("filename", this.container.file.name);
>      formData.append("fileHash", this.container.hash);
>      return { formData,index };
>    })
>    .map(async ({formData,index})=>{
>      this.quest({
>        url:'',
>        data:fromData,
>        requestList: this.requestList
>      }) 
>    })
>    await Promise.all(requestList)
>    // 之前上传的切片数量 + 本次上传的切片数量 = 所有切片数量时
>    // 合并切片
>    if (uploadedList.length + requestList.length === this.data.length) {
>       await this.mergeRequest();
>    }
>  };
>  // 通知服务端合并切片
>  async mergeRequest() {
>        await this.request({
>          url: "http://localhost:3000/merge",
>          headers: {
>            "content-type": "application/json"
>          },
>          data: JSON.stringify({
>            size: SIZE,
>            fileHash: this.container.hash,
>            filename: this.container.file.name
>          })
>        });
>        this.$message.success("上传成功");
>        this.status = Status.wait;
>  },
>  // 根据 hash 验证文件是否曾经已经被上传过
>  // 没有才进行上传
>  async verifyUpload(filename, fileHash) {
>    const { data } = await this.request({
>      url: "http://localhost:3000/verify",
>      headers: {
>        "content-type": "application/json"
>      },
>      data: JSON.stringify({
>        filename,
>        fileHash
>      })
>    });
>    return JSON.parse(data);
>  },
>  ```
>
>  
>
>- 断点续传
>
>  断点续传的原理在于前端/服务端需要`记住`已上传的切片，这样下次上传就可以跳过之前已上传的部分，有两种方案实现记忆的功能
>
>  - 前端的解决方案：前端使用 localStorage 记录已上传的切片 hash
>
>  - - 前端方案有一个缺陷，如果换了个浏览器就失去了记忆的效果，所以这里选取后者
>
>  - 服务端解决方案：服务端保存已上传的切片 hash，前端每次上传前向服务端获取已上传的切片
>
>  断点续传
>
>  - 使用 spark-md5 根据文件内容算出文件 hash
>  - 通过 hash 可以判断服务端是否已经上传该文件，从而直接提示用户上传成功（秒传）
>  - 通过 XMLHttpRequest 的 abort 方法暂停切片的上传
>  - 上传前服务端返回已经上传的切片名，前端跳过这些切片的上传
>
>- 图片的上传过程，get，post，base64，数据流头部内容，图片是如何存储的？比如美国要访问国内的某个图片？CDN！他的工作方式？
>
>  用postman请求接口(图片)的时候，axios里面，responseType默认返回数据类型是json文本形式，将其改为返回数据类型blob (否则，二进制图片数据被强制转换成了json文本形式)。
>
>  ```javascript
>  export function miniprogramQrcode (params) {
>    return axios.post(
>      env.MI_URL + '/XXXX/XXX/XXXX',
>      params,
>      // 将responseType的默认json改为blob
>      {
>      responseType: 'blob',
>      emulateJSON: true
>    }).then(res => {
>      if (res.data) {
>        return Promise.resolve(res.data)
>      } else {
>        throw res
>      }
>    }).catch(err => {
>      return Promise.reject(err)
>    })
>  }
>  ```
>
>  如何处理blob对象，将其显示在前端页面呢？
>
>  ```javascript
>  createMiniQrcode (blob) {
>    let img = document.createElement('img')
>    img.onload = function (e) {
>      // 元素的onload 事件触发后将销毁URL对象, 释放内存。
>      window.URL.revokeObjectURL(img.src)
>    }
>    // 浏览器允许使用URL.createObjectURL()方法，针对 Blob 对象生成一个临时 URL。
>    // 这个 URL 以blob://开头,表明对应一个 Blob 对象。
>    img.src = window.URL.createObjectURL(blob)
>    document.querySelector('.imgQrCode').appendChild(img)
>  }
>  ```
>
>  后端的图片存储方式：（两种）
>
>  1. 后端将图片以独立文件的形式存储在服务器的指定文件夹中，再将图片路径存入数据库字段中; （更常用）
>
>  - - 前端直接将存储路径赋值给src属性即可轻松显示。
>
>  1. 后端将图片转换成二进制流，直接存储到数据库的 Image 类型字段中. （图片不要存储在数据库中）
>
>  - - 前端需要将其二进制流交由blob对象处理，然后通过blob的API生成临时URL赋值给src属性来显示。
>
>  前端的图片显示方式：(三种)
>
>  - url: 一般来说，图片的显示还是建议使用url的方式比较好。如果后端传过来的字段是图片路径的话。
>  - base64：如果图片较大，图片的色彩层次比较丰富，则不适合使用这种方式，因为其Base64编码后的字符串非常大，会明显增大HTML页面，影响加载速度。
>  - blob: 当后端返回特定的图片二进制流的时候，就像我第一part里的情景再现说的，前端用blob容器接收。图片用blob展示会比较好。
>  - 转换：
>
>  - - url转base64
>    - base64转blob
>    - blob转base64
>
>- (CDN)美国要访问国内的某个图片？=> CDN的原理、工作方式
>
>  cdn有用过吗，没用过。。那说下原理呗大概怎么样子的
>
>  [CDN工作原理](https://segmentfault.com/a/1190000000538796)  静态资源上传cdn  
>
>  CDN 内容分发网络（Content Distribution Network）;   DNS 域名服务器（Domain Name System）
>
>  资源上传cdn之后，当用户访问cdn的资源地址之后会经历下面的步骤：
>
>  - 首先经过本地的dns解析，请求cname指向的那台cdn专用的dns服务器。
>  - dns服务器返回全局负载均衡的服务器ip给用户
>  - 用户请求全局负载均衡服务器，服务器根据ip返回所在区域的负载均衡服务器ip给用户
>  - 用户请求区域负载均衡服务器，负载均衡服务器根据用户ip选择距离近的，并且存在用户所需内容的，负载比较合适的一台缓存服务器ip给用户。当没有对应内容的时候，会去上一级缓存服务器去找，直到找到资源所在的源站服务器，并且缓存在缓存服务器中。用户下一次在请求该资源，就可以就近拿缓存了。
>
>  注意：因为cdn的负载均衡和就近选择缓存都是根据用户的ip来的，服务器只能拿到local dns的ip，也就是网络设置中设置的dns ip，如果这个设置的不合理，那么可能起不到加速的效果。可能就近找到的缓存服务器实际离得很远。
>
>  **总结：**
>
>  cdn的原理主要答出负载均衡和缓存再就是dns解析这三部分就行了吧，通过dns解析到全局负载均衡服务器，然后再到区域的负载均衡，之后根据一些条件来找合适的缓存服务器，如果第一次访问就从源站拿过来缓存。需要注意的是一切都是根据请求的ip来的，如果ip不合理，那么可能起不到加速效果。缓存和负载均衡的思想在减轻服务器压力方面其实是很常见的。
>
>- 通过什么做到并发请求
>
>  > 请实现如下的函数，可以批量请求数据，所有的URL地址在urls参数中，同时可以通过max参数控制请求的并发度，当所有请求结束之后，需要执行callback回调函数。发请求的函数可以直接使用fetch即可。
>
>  ```
>  function sendRequest(urls: string[], max: number, callback: () => vois) {}
>  ```
>
>  读题：
>
>  1. 批量请求
>  2. 可控制并发度
>  3. 全部请求结束，执行 `callback`
>
>  解题：
>
>  1. 批量请求
>
>  要实现批量请求，而且并不需要按顺序发起请求（如果需要按顺序可以存入队列中，按优先级则可以存入优先队列中），所以这里我们存入数组中即可，然后进行遍历，取出数字中的每一项丢去`fetch`中进行调用。
>
>  1. 可控制并发度
>
>  本题的难点就在这里！！！在控制并发数的同时，每结束一个请求并发起一个新的请求。依旧使用递归的方式，但这次添加一个请求队列，然后我们只要维护这个队列，每次发起一个请求就添加进去，结束一个就丢出来，继而实现了控制并发。
>
>  1. 全部请求结束，执行 `callback`
>
>  因为是异步请求，我们无法寄希望于安装正常的顺序在函数调用后执行，但是每次`fetch`有返回结果会调用`then`或者`catch`，我们可以在这个时候判断请求数组是否为空就可以知道是否全部被调用完
>
>  ```javascript
>  function handleFetchQueue(urls, max, callback) {
>    const urlCount = urls.length;
>    const requestsQueue = []
>    const results = []
>    let i = 0
>    const handleRequest = (url) => {
>      const req = fetch(url).then(res => {
>        const len = results.push(res).length
>        if (len < urlCount && i + 1 < urlCount) {
>          requestsQueue.shift()
>          handleRequest(urls[++i])
>        } else if (len === urlCount) {
>          typeof callback === 'function' && callback(result)
>        }
>      }).catch(e => {
>        results.push(e)
>      })
>      if (requestsQueue.push(req) < max) {
>        handleRequest(urls[++i])
>      }
>    }
>    handleRequest(urls[i])
>  }
>  
>  const urls = Array.from({length: 10}, (v, k) => k);
>  const fetch = function (idx) {
>    return new Promise(resolve => {
>      console.log(`start request ${idx}`);
>      const timeout = parseInt(Math.random() * 1e4);
>      setTimeout(() => {
>        console.log(`end request ${idx}`);
>        resolve(idx)
>      }, timeout)
>    })
>  };
>  const max = 4;
>  const callback = () => {
>    console.log('run callback');
>  };
>  handleFetchQueue(urls, max, callback);
>  ```